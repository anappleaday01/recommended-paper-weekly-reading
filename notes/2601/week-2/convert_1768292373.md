[[[]()]()]()# Distinctive Feature Codec: An Adaptive Efficient Speech Representation for Depression Detection
<doc2xLabelTranslationLabelBlock>
# 独特特征编解码器：一种用于抑郁症检测的自适应高效语音表示
</doc2xLabelTranslationLabelBlock>


Xiangyu Zhang Student Member, IEEE, Fuming Fang, Peng Gao, Bin Qin, Beena Ahmed Member, IEEE,, Julien Epps Senior Member, IEEE
<doc2xLabelTranslationLabelBlock>
张向宇 学生会员，IEEE，方福明，高鹏，秦斌，比娜·艾哈迈德 会员，IEEE，朱利安·埃普斯 高级会员，IEEE
</doc2xLabelTranslationLabelBlock>


Abstract-Large Language Models (LLMs) have demonstrated remarkable success across diverse fields, establishing a powerful paradigm for complex information processing. This has inspired the integration of speech into LLM frameworks, often by tokenizing continuous audio via neural speech codecs, enabling powerful speech language models. However, this dominant tokenization strategy relies on uniform frame-based processing at fixed time intervals. This fixed-rate approach, while effective for linguistic content, destroys the temporal dynamics. These dynamics are not noise but are established as primary biomarkers in clinical applications such as depression detection. To address this gap, we introduce the Distinctive Feature Codec (DFC), an adaptive framework engineered to preserve this vital timing information. Drawing from linguistic theory, DFC abandons fixed-interval processing and instead learns to dynamically segment the signal at perceptually significant acoustic transitions. This generates variable-length tokens that efficiently encode the temporal structure. As a key contribution, this work is the first to integrate traditional distinctive features into a modern deep learning codec for a temporally sensitive task such as depression detection. We also introduce the Group-wise Scalar Quantization (GSQ) approach to stably quantize these variable-length segments. Our distinctive feature-based approach offers a promising alternative to conventional frame-based processing and advances interpretable representation learning in the modern deep learning speech depression detection framework.
<doc2xLabelTranslationLabelBlock>
摘要 - 大型语言模型（LLMs）在各个领域都取得了显著成功，为复杂信息处理建立了强大的范式。这激发了将语音集成到LLM框架中的尝试，通常是通过神经语音编解码器对连续音频进行分词，从而实现强大的语音语言模型。然而，这种主导的分词策略依赖于在固定时间间隔进行基于均匀帧的处理。这种固定速率方法虽然对语言内容有效，但破坏了时间动态性。这些动态性并非噪声，而是在抑郁症检测等临床应用中被确立为主要生物标志物。为了解决这一差距，我们引入了独特特征编解码器（DFC），这是一个经过精心设计的自适应框架，旨在保留这一重要的时间信息。借鉴语言理论，DFC放弃了固定间隔处理，而是学习在感知上显著的声学过渡处动态分割信号。这生成了可变长度的词元，能够有效地编码时间结构。作为一项关键贡献，这项工作首次将传统独特特征集成到现代深度学习编解码器中，用于抑郁症检测等对时间敏感的任务。我们还引入了分组标量量化（GSQ）方法来稳定量化这些可变长度段。我们基于独特特征的方法为传统基于帧的处理提供了一个有前途的替代方案，并在现代深度学习语音抑郁症检测框架中推进了可解释表示学习。
</doc2xLabelTranslationLabelBlock>


Index Terms-Depression Detection, Tokenization
<doc2xLabelTranslationLabelBlock>
关键词 - 抑郁症检测，分词
</doc2xLabelTranslationLabelBlock>


## I. INTRODUCTION
<doc2xLabelTranslationLabelBlock>
## I. 引言
</doc2xLabelTranslationLabelBlock>


The remarkable success of large language models (LLMs) in understanding and generating text [1], [2], [3] has inspired researchers to develop similar architectures for speech processing [4], [5], [6]. This expansion is motivated by the rich information encoded in speech signals beyond mere linguistic content, including speaker identity, emotion, and prosody [7], [8], [9], [10]. A fundamental challenge in building these speech-aware models is the tokenization of continuous audio into representations that neural networks can process. Unlike text, which has natural boundaries [11], [12], [13], speech is continuous and complex. Consequently, most current approaches-whether for discrete tokenization [14], [15], [16] or self-supervised feature extraction [17], [18]-primarily rely on frame-based processing with fixed time intervals.
<doc2xLabelTranslationLabelBlock>
大型语言模型（LLMs）在理解和生成文本方面取得的显著成功[1], [2], [3]激发了研究人员为语音处理开发类似架构[4], [5], [6]。这种扩展的动机在于语音信号中编码的丰富信息，不仅仅是语言内容，还包括说话者身份、情感和韵律[7], [8], [9], [10]。构建这些语音感知模型的一个基本挑战是将连续音频分词为神经网络可以处理的表示形式。与具有自然边界的文本不同[11], [12], [13]，语音是连续且复杂的。因此，当前大多数方法——无论是用于离散分词[14], [15], [16]还是自监督特征提取[17], [18]——主要依赖于具有固定时间间隔的基于帧的处理。
</doc2xLabelTranslationLabelBlock>


While this fixed-rate processing is effective for tasks focused on linguistic content, such as speech recognition, it presents a fundamental limitation: it overlooks the varying information
<doc2xLabelTranslationLabelBlock>
虽然这种固定速率处理对于专注于语言内容的任务（如语音识别）是有效的，但它存在一个基本限制：它忽略了语音中变化的信息
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



<!-- figureText: Frame-based Processing Distinctive feature-based processing -->



<img src="https://cdn.noedgeai.com/bo_d5ivs477aajc7383hm2g_0.jpg?x=934&y=544&w=704&h=593&r=0"/>



Fig. 1: Comparison of segmentation strategies. The Frame-based processing (top) imposes a rigid grid of fixed-length intervals, often disrupting natural speech events. In contrast, Distinctive feature-based processing (bottom) adaptively segments the signal by identifying perceptually significant acoustic transitions as boundaries. This results in variable-length segments that preserve the fine-grained temporal dynamics essential for clinical tasks
<doc2xLabelTranslationLabelBlock>
图1：分割策略的比较。基于帧的处理（顶部）强加了一个固定长度间隔的刚性网格，常常破坏自然语音事件。相比之下，基于独特特征的处理（底部）通过将感知上显著的声学过渡识别为边界来自适应地分割信号。这导致了可变长度的段，保留了临床任务所需的细粒度时间动态性
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



density of speech and, critically, destroys the fine-grained
<doc2xLabelTranslationLabelBlock>
密度，并且至关重要的是，破坏了细粒度
</doc2xLabelTranslationLabelBlock>


temporal dynamics [19]. This frame-based segmentation arbitrarily cuts through natural speech events, disrupting prosodic rhythms, distorting pause structures, and obscuring speech rate variations. These temporal dynamics are not noise; they are established as primary biomarkers in critical clinical applications, most notably depression detection [20], [21]. Thus, a fundamental mismatch exists: the dominant speech representation methods are optimized for linguistic content, making them inherently unsuitable for downstream tasks that depend on temporal fidelity.

时间动态性[19]。这种基于帧的分割任意地切断自然语音事件，扰乱韵律节奏，扭曲停顿结构，并模糊语速变化。这些时间动态性并非噪声；它们在关键临床应用中被确立为主要生物标志物，最显著的是抑郁症检测[20], [21]。==因此，存在一个基本的不匹配：主导的语音表示方法是针对语言内容进行优化的，这使得它们本质上不适合依赖时间保真度的下游任务。==



To bridge this gap, we must identify meaningful units within continuous signals in a way that preserves temporal structure. The linguistic theory of distinctive features provides an insightful alternative [22]. This theory posits that speech can be naturally segmented at points where acoustic characteristics are most differentiated [23], [24]. Instead of arbitrary fixed-length segments, this approach identifies boundaries where acoustic properties undergo significant changes. Such an adaptive, variable-length segmentation process naturally encodes the temporal dynamics; for example, a pause is no longer a sequence of identical "silence" frames, but is represented by a single, long segment whose duration is explicitly preserved. However, implementing distinctive feature analysis in neural networks has remained challenging. The irregular, variable-length nature of these features conflicts with the regular, grid-like computations (e.g., convolutions, transformers) that dominate modern deep learning [25], [26], [17], [18]. This technical mismatch has largely confined distinctive feature analysis to traditional signal processing approaches [19], [27].

为了弥合这一差距，我们必须以一种保留时间结构的方式在连续信号中识别有意义的单元。独特特征的语言理论提供了一个有见地的替代方案[22]。==该理论认为，语音可以在声学特征差异最大的点自然分割[23], [24]。==这种方法不是任意的固定长度段，而是识别声学属性发生显著变化的边界。这样一个自适应、可变长度的分割过程自然地编码了时间动态性；例如，一个停顿不再是一系列相同的“静音”帧，而是由一个单一的长段表示，其持续时间被明确保留。==然而，在神经网络中实现独特特征分析仍然具有挑战性。这些特征不规则、可变长度的性质与主导现代深度学习的规则、网格状计算（如卷积、变压器）相冲突[25], [26], [17], [18]==。这种技术不匹配在很大程度上限制了独特特征分析仅应用于传统信号处理方法[19], [27]。



---

<!-- Footnote -->



Xiangyu Zhang, Beena Ahmed, Julien Epps are with the School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, Australia.
<doc2xLabelTranslationLabelBlock>
张向宇、比娜·艾哈迈德、朱利安·埃普斯就职于澳大利亚悉尼新南威尔士大学电气工程与电信学院。
</doc2xLabelTranslationLabelBlock>


Fuming Fang, Peng Gao, Bin Qin are with Xiaomi Corp, Beijing, China.
<doc2xLabelTranslationLabelBlock>
方福明、高鹏、秦斌就职于中国北京小米公司。
</doc2xLabelTranslationLabelBlock>


<!-- Footnote -->

---



In this paper, we address this challenge by introducing the Distinctive Feature Codec (DFC), a framework designed to learn an efficient speech representation that preserves the critical temporal information required for depression detection. We propose the first architecture that successfully integrates the theory of distinctive features into a modern, end-to-end neural codec framework [28], [29]. Our approach uses a lightweight, self-supervised boundary detector to identify perceptually significant acoustic transitions, which guide an adaptive encoder-decoder (based on SEANet [30]) to process variable-length segments. This marks a departure from conventional fixed-interval processing [14], [15], [16]. Furthermore, our investigation reveals that standard quantization methods like Finite Scalar Quantization (FSQ) [31] become unstable when applied to such variable-length segments at low bitrates. To solve this, we develop a novel Group-wise Scalar Quantization (GSQ) approach, which ensures robust and stable quantization. Our work validates distinctive features as a promising direction for codec design, offering new perspectives on efficient speech representation for depression detection.

在本文中，我们通过引入独特特征编解码器（DFC）来应对这一挑战，DFC是一个旨在学习高效语音表示的框架，该表示保留了抑郁症检测所需的关键时间信息。我们提出了首个成功将独特特征理论集成到现代端到端神经编解码器框架中的架构[28],[29]。我们的方法==使用轻量级的自监督边界检测器来识别感知上显著的声学转变，这引导一个自适应编码器-解码器（基于SEANet[30]）来处理可变长度的片段。==这标志着与传统的固定间隔处理[14],[15],[16]有所不同。此外，我们的研究表明，像有限标量量化（FSQ）[31]这样的**标准量化方法在低比特率下应用于此类可变长度片段时会变得不稳定**。为了解决这个问题，我们==开发了一种新颖的分组标量量化（GSQ）方法，该方法确保了稳健且稳定的量化。==我们的工作验证了独特特征作为编解码器设计的一个有前景的方向，为抑郁症检测的高效语音表示提供了新的视角。



## II. Preliminary
<doc2xLabelTranslationLabelBlock>
## 二、预备知识
</doc2xLabelTranslationLabelBlock>


## A. Distinctive Features
<doc2xLabelTranslationLabelBlock>
## A. 独特特征
</doc2xLabelTranslationLabelBlock>


Distinctive features, first proposed in linguistic theory [22], [23], [32], [24], characterize speech by identifying regions with acoustically distinctive properties that help differentiate speech segments from one another. As illustrated in Fig.1, this approach fundamentally differs from conventional frame-based processing: while frame-based methods uniformly segment speech signals into fixed-length overlapping windows, distinctive feature analysis identifies boundaries where acoustic characteristics undergo significant changes. This approach has proven valuable in early automatic speech recognition systems [23], [32], [33] and has been successfully applied to various healthcare applications [34], [35], [36], [21], [37]. However, despite its theoretical advantages, the development of distinctive feature-based methods has faced significant limitations in the deep learning era. Traditional implementations of distinctive features heavily rely on linguistic expertise and hand-crafted rules [23], [24], leading to limited training data and difficulties in scaling across different acoustic conditions and languages. Additionally, while frame-level processing naturally aligns with convolutional neural networks and enables efficient batch processing, the variable-length nature of distinctive features poses challenges for modern deep learning architectures. The success of frame-level processing in various deep learning systems [17], [18] has led to a rich ecosystem of pre-trained models and established practices, making it the predominant choice for modern speech processing systems despite its inherent inefficiencies. This technical mismatch, combined with the limited availability of labeled data for distinctive feature analysis, has constrained its adoption in contemporary deep learning approaches.

==独特特征==最早在语言学理论[22],[23],[32],[24]中被提出，它通过识别具有声学独特属性的区域来表征语音，这些属性有助于区分不同的语音片段。如图1所示，这种方法与传统的基于帧的处理方法有根本区别：基于帧的方法将语音信号均匀地分割成固定长度的重叠窗口，而独特特征分析则识别声学特征发生显著变化的边界。这种方法在早期的自动语音识别系统[23],[32],[33]中已被证明是有价值的，并已成功应用于各种医疗保健应用[34],[35],[36],[21],[37]。然而，尽管其具有理论优势，但在深度学习时代，基于独特特征的方法的发展面临着重大限制。独特特征的**传统实现严重依赖语言学专业知识和手工制作的规则**[23],[24]，导致训练数据有限，并且在不同声学条件和语言之间进行扩展时存在困难。此外，虽然帧级处理自然地与卷积神经网络对齐并实现高效的批量处理，但独特特征的可变长度性质给现代深度学习架构带来了挑战。**帧级处理在各种深度学习系统[17],[18]中的成功导致了丰富的预训练模型生态系统和既定实践，尽管其存在固有的低效率，但它已成为现代语音处理系统的主要选择**。这种技术不匹配，再加上用于独特特征分析的标记数据有限，限制了其在当代深度学习方法中的采用。



<!-- Media -->



TABLE I: Analysis of Sylber Codec performance. The table shows mel_error, stft, pseq, and stoi metrics for different KM (K-Means) cluster sizes.
<doc2xLabelTranslationLabelBlock>
表一：Sylber编解码器性能分析。该表展示了不同KM（K-Means）聚类大小下的mel_error、stft、pseq和stoi指标。
</doc2xLabelTranslationLabelBlock>


<table><tr><td>Sylber Codec</td><td>mel_error \( \downarrow \)</td><td>stft↓</td><td>PESQ↑</td><td>stoi↑</td></tr><tr><td>KM=5K</td><td>0.6204</td><td>2.0246</td><td>1.0025</td><td>0.7066</td></tr><tr><td>KM=10K</td><td>0.6155</td><td>2.2049</td><td>1.2501</td><td>0.7136</td></tr><tr><td>KM = 20K</td><td>0.6727</td><td>2.1525</td><td>0.8135</td><td>0.7003</td></tr></table>
<doc2xLabelTranslationLabelBlock>
<table><tbody><tr><td>西尔伯编解码器</td><td>梅尔误差\( \downarrow \)</td><td>短时傅里叶变换↓</td><td>语音质量感知评估↑</td><td>短时目标可懂度↑</td></tr><tr><td>知识图谱=5K</td><td>0.6204</td><td>2.0246</td><td>1.0025</td><td>0.7066</td></tr><tr><td>知识图谱=10K</td><td>0.6155</td><td>2.2049</td><td>1.2501</td><td>0.7136</td></tr><tr><td>\( \mathrm{{KM}} = {20}\mathrm{\;K} \)</td><td>0.6727</td><td>2.1525</td><td>0.8135</td><td>0.7003</td></tr></tbody></table>
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



## B. Speech Codecs
<doc2xLabelTranslationLabelBlock>
## B. 语音编解码器
</doc2xLabelTranslationLabelBlock>


Speech codecs compress speech signals into discrete tokens while preserving essential acoustic and linguistic information [38], [39]. These discrete representations serve as inputs for downstream tasks such as large language models [5], [6], [4]. Current approaches can be broadly categorized into two paradigms: end-to-end trained codecs that directly learn discrete representations through reconstruction objectives [15], [14], [16], and two-stage approaches that first extract semantic features using self-supervised models [18], [17], then apply generative modeling techniques such as flow matching [40], [41] or diffusion [42], [43].

==语音编解码器将语音信号压缩为离散令牌==，同时保留基本的声学和语言信息[38],[39]。这些离散表示==用作下游任务（如大语言模型[5],[6],[4]）的输入==。当前方法大致==可分为两种范式==：通==过重建目标直接学习离散表示的端到端训练编解码器[15],[14],[16]，以及首先使用自监督模型提取语义特征[18],[17]，然后应用生成建模技术（如流匹配[40],[41]或扩散[42],[43]）的两阶段方法==。



Two-stage approaches (e.g., Sylber [44]) achieve impressive compression by operating at syllable-level granularity [45], [46], reducing token rates to as low as 5-10 tokens per second. These methods prioritize semantic preservation-retaining linguistic content that overlaps with text representations-while relying on powerful generative models. However, this design philosophy is fundamentally misaligned with depression detection, where diagnostic information resides not in semantic content but in acoustic characteristics: voice quality, spectral energy distribution, and prosodic micro-variations [21].

两阶段方法（例如Sylber[44]）通过在音节级粒度上操作[45],[46]实现了令人印象深刻的压缩，将令牌率降低到低至每秒5 - 10个令牌。这些方法在依赖强大生成模型的同时，==优先保留语义——保留与文本表示重叠的语言内容==。然而，这种设计理念==与抑郁症检测从根本上不一致，在抑郁症检测中，诊断信息不在于语义内容，而在于声学特征==：语音质量、频谱能量分布和韵律微变化[21]。



Table I, based on our experimental evaluation of the Sylber model, empirically demonstrates this limitation. As Sylber's model capacity increases (K-means clusters from \( {10}\mathrm{\;K} \) to \( {20}\mathrm{\;K} \) ), perceptual quality (PESQ) paradoxically degrades from 1.25 to 0.81 , while mel-spectral error increases from 0.616 to 0.673 . This pattern indicates that the generative model produces perceptually plausible speech by hallucinating acoustic details rather than preserving them from the original signal. While the semantic content ("what was said") remains intact, the acoustic substrate ("how it was said") diverges from the input-erasing the subtle spectral and prosodic deviations that constitute depression biomarkers. Multiple studies confirm that such semantically-focused representations underperform on tasks requiring acoustic fidelity [47], [48].

表I基于我们对Sylber模型的实验评估，从经验上证明了这一局限性。随着Sylber模型容量增加（K均值聚类从\( ${10}\mathrm{\;K}$ \)到\( ${20}\mathrm{\;K}$\)），感知质量（PESQ）自相矛盾地从1.25下降到0.81，而梅尔频谱误差从0.616增加到0.673。这种模式==表明生成模型通过虚构声学细节而不是从原始信号中保留它们来产生感知上合理的语音==。虽然语义内容（“说了什么”）保持不变，但声学基础（“怎么说的”）与输入不同——抹去了构成抑郁症生物标志物的微妙频谱和韵律偏差。多项研究证实，这种**以语义为重点的表示在需要声学保真度的任务上表现不佳**[47],[48]。



Our work therefore focuses on end-to-end trained codecs, which optimize for faithful acoustic reconstruction rather than semantic compression, explicitly preserving the spectral and temporal dynamics essential for clinical applications.

因此，我们的工作专注于端到端训练的编解码器，它==针对忠实的声学重建而不是语义压缩进行优化==，明确保留临床应用所需的频谱和时间动态。



<!-- Media -->



<!-- figureText: Japosus miro Distinctive Feature Detect japowia -->



<img src="https://cdn.noedgeai.com/bo_d5ivs477aajc7383hm2g_2.jpg?x=300&y=145&w=1219&h=738&r=0"/>



Fig. 2: Overview of the Distinctive Feature Codec framework. The Distinctive Feature Detector (top) identifies acoustic boundaries through contrastive learning to guide variable-length segmentation. The codec pipeline segments encoded speech based on these boundaries, compresses each segment via the DF Encoder, applies quantization, and reconstructs through the DF Decoder and main decoder.
<doc2xLabelTranslationLabelBlock>
图2：独特特征编解码器框架概述。独特特征检测器（顶部）通过对比学习识别声学边界以指导可变长度分割。编解码器管道基于这些边界对编码语音进行分割，通过DF编码器压缩每个段，应用量化，并通过DF解码器和主解码器进行重建。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



## C. Speech Representation for Depression Detection
<doc2xLabelTranslationLabelBlock>
## C. 用于抑郁症检测的语音表示
</doc2xLabelTranslationLabelBlock>


Clinical research has long established that depression profoundly affects speech production mechanisms, manifesting in distinctive acoustic biomarkers known as "psychomotor retardation" [49]. These manifestations primarily include reduced speech rate, monotonic prosody, prolonged pauses, and blunted articulation [50], [51]. Consequently, capturing these temporal dynamics and prosodic variations has been a central focus in feature extraction for depression detection.
<doc2xLabelTranslationLabelBlock>
长期以来，临床研究已经确定抑郁症会深刻影响语音产生机制，表现为被称为“精神运动迟缓”的独特声学生物标志物[49]。这些表现主要包括语速降低、韵律单调、停顿延长和发音模糊[50],[51]。因此，捕捉这些时间动态和韵律变化一直是抑郁症检测特征提取的核心重点。
</doc2xLabelTranslationLabelBlock>


Early approaches predominantly relied on hand-crafted Low-Level Descriptors (LLDs) to explicitly model these characteristics. Standard feature sets, such as eGeMAPS and INTERSPEECH ComParE [52], aggregate frame-level acoustic properties (e.g., pitch, energy, formants) using statistical functionals to capture global prosodic trends. More distinctively, some studies explored landmark-based or distinctive feature analysis [20], [37], which focuses on specific acoustic events-such as the onset of bursts or glottal transitions-to detect subtle articulatory coordination deficits associated with depression [20], [34], [21]. These methods offered high interpretability and preserved the temporal integrity of speech events but were limited by their inability to model complex, high-level abstractions.

早期方法主要依赖手工制作的低级描述符（LLD）来明确建模这些特征。标准特征集，如eGeMAPS和INTERSPEECH ComParE[52]，使用统计泛函聚合帧级声学属性（例如音高、能量、共振峰）以捕捉全局韵律趋势。更独特的是，一些研究探索了基于地标或独特特征分析[20],[37]，其专注于特定声学事件——如爆发的开始或声门转换——以检测与抑郁症相关的微妙发音协调缺陷[20],[34],[21]。这些方法具有很高的可解释性，并保留了语音事件的时间完整性，但==受到无法对复杂的高级抽象进行建模的限制==。



In the deep learning era, the paradigm shifted towards learning data-driven representations directly from raw audio or spectrograms using Convolutional Neural Networks (CNNs) and Transformers [25], [26]. Self-supervised models (SSL) like Wav2Vec 2.0 and HuBERT have achieved state-of-the-art performance by encoding speech into continuous or discrete representations [17], [18], [15], [16]. However, these modern approaches predominantly employ fixed-rate frame-based processing, where speech is segmented into uniform time intervals regardless of the underlying acoustic content. While effective for linguistic tasks like ASR, recent studies suggest that this rigid segmentation may disrupt the fine-grained temporal structures-such as variable-length pauses and rhythmic patterns—that serve as critical diagnostic cues [19], [37], [21]. This creates a fundamental mismatch: the dominant feature extraction methods are optimized for semantic continuity, potentially at the cost of the temporal fidelity required for reliable depression assessment.

在深度学习时代，范式转向使用卷积神经网络（CNN）和Transformer[25],[26]直接从原始音频或频谱图学习数据驱动的表示。像Wav2Vec 2.0和HuBERT这样的自监督模型（SSL）通过将语音编码为连续或离散表示[17],[18],[15],[16]取得了领先的性能。然而，这些现代方法主要采用固定速率的基于帧的处理，其中语音被分割成均匀的时间间隔，而不管潜在的声学内容如何。虽然对像ASR这样的语言任务有效，但最近的研究表明这种==刚性分割可能会破坏作为关键诊断线索的细粒度时间结构==——如可变长度的停顿和节奏模式[19],[37],[21]。这造成了一个==根本的不匹配：主要的特征提取方法是为语义连续性优化的，可能以可靠抑郁症评估所需的时间保真度为代价==。



## III. DISTINCTIVE CODEC FRAMEWORK
<doc2xLabelTranslationLabelBlock>
## III. 独特编解码器框架
</doc2xLabelTranslationLabelBlock>


As illustrated in Fig. 2, our approach consists of two key stages: First, a lightweight boundary detector is trained to identify perceptually significant transitions in speech signals. Second, these detected boundaries guide the codec model to adaptively merge or separate speech segments, leading to more efficient tokenization that aligns with the natural structure of speech.
<doc2xLabelTranslationLabelBlock>
如图2所示，我们的方法包括两个关键阶段：第一，训练一个轻量级边界检测器，以识别语音信号中在感知上显著的过渡。第二，这些检测到的边界指导编解码器模型自适应地合并或分离语音段，从而实现更高效的词元化，使其与语音的自然结构对齐。
</doc2xLabelTranslationLabelBlock>


## A. Distinctive Features Detector
<doc2xLabelTranslationLabelBlock>
## A. 独特特征检测器
</doc2xLabelTranslationLabelBlock>


The core idea of distinctive features lies in identifying regions where speech segments exhibit maximal acoustic contrast with their neighbors. This naturally aligns with the objective of contrastive learning, which aims to learn representations by maximizing the differences between distinct samples while minimizing differences between similar ones [53], [54], [55], [56]. We leverage this connection to design a self-supervised boundary detector that learns to identify distinctive features without requiring phoneme-level annotations.

==独特特征的核心思想在于识别语音段与其相邻段呈现最大声学对比度的区域==。这自然**与对比学习的目标一致**，对比学习旨在通过最大化不同样本之间的差异，同时最小化相似样本之间的差异来学习表示 [53], [54], [55], [56]。我们利用这种联系设计了一个自监督边界检测器，该检测器无需音素级注释就能学习识别独特特征。



Specifically,we train a lightweight encoder network \( {f}_{\theta } \) : \( {\mathbb{R}}^{L} \rightarrow  {\mathbb{R}}^{D} \) that maps raw speech input segments \( \mathbf{x} = \) \( \left\{  {{x}_{1},\ldots ,{x}_{T}}\right\} \) into a latent representation space,where \( L \) is the segment length and \( D \) is the latent dimension. For a given segment \( {x}_{t} \) at position \( t \) ,we compute its latent representation \( {\mathbf{z}}_{t} = {f}_{\theta }\left( {x}_{t}\right) \) and compare it with subsequent segments at different positions \( t + {k}_{k = 1}^{K} \) . The similarity score is computed along the feature dimension:

具体来说，我们训练一个轻量级编码器网络\( ${f}_{\theta }$ \) : \( ${\mathbb{R}}^{L} \rightarrow  {\mathbb{R}}^{D}$ \)，它将原始语音输入段\( \mathbf{x} = \) \( \left\{  {{x}_{1},\ldots ,{x}_{T}}\right\} \)映射到一个潜在表示空间，其中\( L \)是段长度，\( D \)是潜在维度。对于位置\( t \)处的给定段\( {x}_{t} \)，我们计算其潜在表示\( {\mathbf{z}}_{t} = {f}_{\theta }\left( {x}_{t}\right) \)，并将其与不同位置\( t + {k}_{k = 1}^{K} \)的后续段进行比较。相似性得分沿特征维度计算：



$\[s\left( {t,k}\right)  =  - \alpha  \cdot  \cos {\left( {\mathbf{z}}_{t},{\mathbf{z}}_{t + k}\right) }_{D} \tag{1}\]$



where \( \alpha \) is a scaling coefficient. For each positive pair \( \left( {{x}_{t},{x}_{t + k}}\right) \) ,we construct a set of negative samples by randomly shuffling segments from the same batch. The model is trained to minimize the contrastive loss:
<doc2xLabelTranslationLabelBlock>
其中\( \alpha \)是一个缩放系数。对于每个正样本对\( \left( {{x}_{t},{x}_{t + k}}\right) \)，我们通过随机打乱同一批次中的段来构造一组负样本。训练模型以最小化对比损失：
</doc2xLabelTranslationLabelBlock>


\[$\mathcal{L} =  - {\mathbb{E}}_{t,k}\left\lbrack  {\log \frac{\exp \left( {s\left( {t,k}\right) /\tau }\right) }{\exp \left( {s\left( {t,k}\right) /\tau }\right)  + \mathop{\sum }\limits_{{n = 1}}^{N}\exp \left( {s\left( {t,n}\right) /\tau }\right) }}\right\rbrack   \tag{2}$\]



where \( \tau \) is a temperature parameter and \( N \) is the number of negative samples. This contrastive objective encourages the model to learn representations that capture the inherent acoustic differences between speech segments. The resulting similarity scores naturally highlight regions where acoustic characteristics undergo significant changes, corresponding to distinctive feature boundaries. These boundaries then guide the subsequent merging of segments in our codec model.
<doc2xLabelTranslationLabelBlock>
其中\( \tau \)是温度参数，\( N \)是负样本数量。这个对比目标鼓励模型学习能够捕捉语音段之间固有声学差异的表示。由此产生的相似性得分自然会突出声学特征发生显著变化的区域，这些区域对应于独特特征边界。然后这些边界指导我们编解码器模型中后续的段合并。
</doc2xLabelTranslationLabelBlock>


## B. Distinctive Codec
<doc2xLabelTranslationLabelBlock>
## B. 独特编解码器
</doc2xLabelTranslationLabelBlock>


To evaluate our distinctive feature-based approach, we build our codec model on top of the SpeechTokenizer framework [16], which leverages the SEANet architecture [30] for encoder-decoder operations. The key innovation of our approach lies in how we process the encoded representations based on the distinctive features detected by our boundary detector. Given input speech \( \mathbf{x} \in  {\mathbb{R}}^{1 \times  L} \) ,the encoder \( {E}_{\theta } \) first maps it to a latent representation:
<doc2xLabelTranslationLabelBlock>
为了评估我们基于独特特征的方法，我们在SpeechTokenizer框架 [16] 之上构建我们的编解码器模型，该框架利用SEANet架构 [30] 进行编码器 - 解码器操作。我们方法的关键创新在于基于我们的边界检测器检测到的独特特征来处理编码表示的方式。给定输入语音\( \mathbf{x} \in  {\mathbb{R}}^{1 \times  L} \)，编码器\( {E}_{\theta } \)首先将其映射到一个潜在表示：
</doc2xLabelTranslationLabelBlock>


\[\mathbf{e} = {E}_{\theta }\left( \mathbf{x}\right)  \in  {\mathbb{R}}^{D \times  T} \tag{3}\]



where \( D \) is the feature dimension and \( T = L/r \) represents the temporal dimension after downsampling with ratio \( r \) . Traditional frame-based codecs would typically process this representation uniformly across time. In contrast, our distinctive codec first identifies segment boundaries \( \mathcal{B} = {b}_{1},{b}_{2},\ldots ,{b}_{M} \) using the boundary detector described in Section III-A. With these boundaries, we partition the feature sequence into variable-length segments:
<doc2xLabelTranslationLabelBlock>
其中\( D \)是特征维度，\( T = L/r \)表示以比率\( r \)下采样后的时间维度。传统的基于帧的编解码器通常会在整个时间上均匀地处理这种表示。相比之下，我们的独特编解码器首先使用第三节 - A中描述的边界检测器识别段边界\( \mathcal{B} = {b}_{1},{b}_{2},\ldots ,{b}_{M} \)。利用这些边界，我们将特征序列划分为可变长度的段：
</doc2xLabelTranslationLabelBlock>


\[\mathbf{S} = {\mathbf{s}}_{1},{\mathbf{s}}_{2},\ldots ,{\mathbf{s}}_{M + 1} \tag{4}\]



where \( {\mathbf{s}}_{i} = \mathbf{e}\left\lbrack  { : ,{b}_{i - 1} : {b}_{i}}\right\rbrack \) represents the \( i \) -th segment \( \left( {{b}_{0} = 0}\right. \) and \( {b}_{M + 1} = T \) for notational convenience). For each segment, we apply a Distinctive Feature encoder (DFE) to compress the variable-length representation into a fixed-length embedding:
<doc2xLabelTranslationLabelBlock>
其中\( {\mathbf{s}}_{i} = \mathbf{e}\left\lbrack  { : ,{b}_{i - 1} : {b}_{i}}\right\rbrack \)表示第\( i \)个片段\( \left( {{b}_{0} = 0}\right. \)，为了符号表示方便，这里用\( {b}_{M + 1} = T \)表示。对于每个片段，我们应用一个独特特征编码器（DFE）将可变长度表示压缩为固定长度嵌入：
</doc2xLabelTranslationLabelBlock>


\[{\mathbf{z}}_{i} = \operatorname{DFE}\left( {\mathbf{s}}_{i}\right)  \in  {\mathbb{R}}^{H \times  1} \tag{5}\]



where \( H \) is the hidden dimension. This operation effectively merges temporal information within each segment into a single token, guided by the distinctive feature boundaries. During decoding, we expand each compressed segment embedding back to its original length using a Distinctive Feature decoder (DFD):
<doc2xLabelTranslationLabelBlock>
其中\( H \)是隐藏维度。此操作有效地将每个段内的时间信息合并为单个词元，由独特特征边界引导。在解码期间，我们使用独特特征解码器（DFD）将每个压缩段嵌入扩展回其原始长度：
</doc2xLabelTranslationLabelBlock>


\[{\widehat{\mathbf{s}}}_{i} = \operatorname{DFD}\left( {{\mathbf{z}}_{i},{l}_{i}}\right)  \tag{6}\]



<!-- Media -->



<!-- figureText: Quantizer Sub-Dimension Sub-Dimension FSQ FSQ Sub-Dimension Sub-Dimension FSQ -->



<img src="https://cdn.noedgeai.com/bo_d5ivs477aajc7383hm2g_3.jpg?x=988&y=166&w=595&h=435&r=0"/>



Fig. 3: Features are divided into smaller groups for independent quantization, enhancing stability and representation quality.
<doc2xLabelTranslationLabelBlock>
图3：特征被分成较小的组进行独立量化，增强了稳定性和表示质量。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



where \( {l}_{i} = {b}_{i} - {b}_{i - 1} \) is the original segment length. The full sequence is reconstructed by concatenating the expanded segments:
<doc2xLabelTranslationLabelBlock>
其中\( {l}_{i} = {b}_{i} - {b}_{i - 1} \)是原始段长度。通过连接扩展后的段来重建完整序列：
</doc2xLabelTranslationLabelBlock>


\[\widehat{\mathbf{e}} = \operatorname{Concat}\left( {{\widehat{\mathbf{s}}}_{1},{\widehat{\mathbf{s}}}_{2},\ldots ,{\widehat{\mathbf{s}}}_{M + 1}}\right)  \tag{7}\]



Finally,the decoder \( {D}_{\phi } \) transforms the reconstructed latent representation back to the waveform domain:
<doc2xLabelTranslationLabelBlock>
最后，解码器\( {D}_{\phi } \)将重建的潜在表示转换回波形域：
</doc2xLabelTranslationLabelBlock>


\[\widehat{\mathbf{x}} = {D}_{\phi }\left( \widehat{\mathbf{e}}\right)  \tag{8}\]



## C. Group-wise Scalar Quantization
<doc2xLabelTranslationLabelBlock>
## C. 分组标量量化
</doc2xLabelTranslationLabelBlock>


Finite Scalar Quantization (FSQ) [31] has emerged as an effective approach for discrete representation learning due to its computational efficiency and strong performance across various tasks [6], [57], [58]. Unlike vector quantizers that require nearest neighbor search in high-dimensional spaces, FSQ directly quantizes each dimension of the latent representation independently, significantly reducing computational complexity.
<doc2xLabelTranslationLabelBlock>
有限标量量化（FSQ）[31]由于其计算效率和在各种任务中的强大性能[6]、[57]、[58]，已成为离散表示学习的一种有效方法。与需要在高维空间中进行最近邻搜索的矢量量化器不同，FSQ直接独立地对潜在表示的每个维度进行量化，显著降低了计算复杂度。
</doc2xLabelTranslationLabelBlock>


However, during our experiments with distinctive feature-based tokenization, we discovered that standard FSQ becomes unstable when operating at high downsampling rates. This instability manifests as training divergence and poor reconstruction quality, particularly when compressing longer, variable-length segments into single tokens. We hypothesize that this issue stems from the increased difficulty of directly quantizing high-dimensional features with varying temporal characteristics.
<doc2xLabelTranslationLabelBlock>
然而，在我们基于独特特征的分词实验中，我们发现标准FSQ在高下采样率下运行时会变得不稳定。这种不稳定性表现为训练发散和重建质量差，特别是在将较长的可变长度段压缩为单个令牌时。我们推测这个问题源于直接量化具有不同时间特征的高维特征的难度增加。
</doc2xLabelTranslationLabelBlock>


To address this issue, we propose Group-wise Scalar Quantization (GSQ) as shown in Figure 3, which decomposes the high-dimensional quantization problem into multiple lower-dimensional sub-problems. Given a compressed segment representation \( {\mathbf{z}}_{i} \in  {\mathbb{R}}^{H \times  1} \) ,we divide it into \( G \) groups,each with dimension \( {H}_{g} = H/G \) :
<doc2xLabelTranslationLabelBlock>
为了解决这个问题，我们提出了如图3所示的分组标量量化（GSQ），它将高维量化问题分解为多个低维子问题。给定一个压缩段表示\( {\mathbf{z}}_{i} \in  {\mathbb{R}}^{H \times  1} \)，我们将其划分为\( G \)组，每组维度为\( {H}_{g} = H/G \)：
</doc2xLabelTranslationLabelBlock>


\[{\mathbf{z}}_{i} = \left\lbrack  {{\mathbf{z}}_{i}^{1},{\mathbf{z}}_{i}^{2},\ldots ,{\mathbf{z}}_{i}^{G}}\right\rbrack   \tag{9}\]



Each group is then processed independently through quantization. The key design choice lies in how we parameterize the quantization function for each group \( {Q}_{g}\left( \cdot \right) \) . We explore two variants:
<doc2xLabelTranslationLabelBlock>
然后通过量化对每个组进行独立处理。关键的设计选择在于我们如何为每个组\( {Q}_{g}\left( \cdot \right) \)参数化量化函数。我们探索了两种变体：
</doc2xLabelTranslationLabelBlock>


Direct Quantization (M2M): Each group is quantized directly without dimensional change:
<doc2xLabelTranslationLabelBlock>
直接量化（M2M）：每个组直接量化，不改变维度：
</doc2xLabelTranslationLabelBlock>


\[{\widehat{\mathbf{z}}}_{i}^{g} = {\operatorname{FSQ}}_{g}\left( {\mathbf{z}}_{i}^{g}\right) ,\;{\mathbf{z}}_{i}^{g},{\widehat{\mathbf{z}}}_{i}^{g} \in  {\mathbb{R}}^{{H}_{g}} \tag{10}\]



This preserves the distributed representation across all \( {H}_{g} \) dimensions but requires a codebook of size \( {L}^{{H}_{g}} \) per group (where \( L \) is the number of quantization levels per dimension). Projection-based Quantization (M2O): Each group is first projected to a lower-dimensional space (in our case, a scalar), quantized, then projected back:
<doc2xLabelTranslationLabelBlock>
这保留了所有\( {H}_{g} \)维度上的分布式表示，但每组需要一个大小为\( {L}^{{H}_{g}} \)的码本（其中\( L \)是每个维度的量化级别数）。基于投影的量化（M2O）：每个组首先投影到一个低维空间（在我们的例子中是一个标量），量化，然后再投影回来：
</doc2xLabelTranslationLabelBlock>


\[{p}_{i}^{g} = {\mathbf{W}}_{g}^{\left( c\right) }{\mathbf{z}}_{i}^{g},\;{\widehat{p}}_{i}^{g} = {\mathrm{{FSQ}}}_{g}\left( {p}_{i}^{g}\right) ,\;{\widehat{\mathbf{z}}}_{i}^{g} = {\mathbf{V}}_{g}^{\left( e\right) }{\widehat{p}}_{i}^{g} \tag{11}\]



where \( {\mathbf{W}}_{g}^{\left( c\right) } \in  {\mathbb{R}}^{k \times  {H}_{g}} \) is the compression matrix, \( {\mathbf{V}}_{g}^{\left( e\right) } \in \) \( {\mathbb{R}}^{{H}_{g} \times  k} \) is the expansion matrix,and \( k \) is the projection dimension (we use \( k = 1 \) ,i.e.,scalar projection). This reduces the codebook size to \( {L}^{k} \) per group while maintaining expressiveness through learned projections.
<doc2xLabelTranslationLabelBlock>
其中\( {\mathbf{W}}_{g}^{\left( c\right) } \in  {\mathbb{R}}^{k \times  {H}_{g}} \)是压缩矩阵，\( {\mathbf{V}}_{g}^{\left( e\right) } \in \)\( {\mathbb{R}}^{{H}_{g} \times  k} \)是扩展矩阵，\( k \)是投影维度（我们使用\( k = 1 \)，即标量投影）。这将每组的码本大小减少到\( {L}^{k} \)，同时通过学习到的投影保持表达能力。
</doc2xLabelTranslationLabelBlock>


The full representation is reconstructed by concatenating all quantized groups:
<doc2xLabelTranslationLabelBlock>
通过连接所有量化组来重建完整表示：
</doc2xLabelTranslationLabelBlock>


\[{\widehat{\mathbf{z}}}_{i} = \left\lbrack  {{\widehat{\mathbf{z}}}_{i}^{1},{\widehat{\mathbf{z}}}_{i}^{2},\ldots ,{\widehat{\mathbf{z}}}_{i}^{G}}\right\rbrack   \tag{12}\]



This group-wise decomposition is particularly beneficial for quantizing variable-length segments with diverse acoustic characteristics. By processing features in smaller groups with dedicated quantizers, we reduce quantization complexity through lower-dimensional operations. Our primary experiments employ the M2O variant with scalar projection \( \left( {k = 1}\right) \) due to its superior stability at low token rates and dramatically reduced codebook size. Algorithm 1 presents the unified framework,with the projection step (lines 3-4,6) being optional for the M2M variant.
<doc2xLabelTranslationLabelBlock>
这种分组分解对于量化具有不同声学特征的可变长度段特别有益。通过使用专用量化器在较小的组中处理特征，我们通过低维操作降低了量化复杂度。我们的主要实验采用具有标量投影\( \left( {k = 1}\right) \)的M2O变体，因为它在低令牌率下具有卓越的稳定性，并且码本大小显著减小。算法1展示了统一框架，对于M2M变体，投影步骤（第3 - 4行、第6行）是可选的。
</doc2xLabelTranslationLabelBlock>


## IV. EXPERIMENTAL SETUP
<doc2xLabelTranslationLabelBlock>
## 四、实验设置
</doc2xLabelTranslationLabelBlock>


We implement our Distinctive Codec using the widely-adopted SEANet-based [30] encoder-decoder architecture, which has become the de facto standard in modern neural speech codecs including SoundStream [15], EnCodec [59], DAC [59], and SpeechTokenizer [16]. This architectural choice enables direct comparison with existing methods while isolating the impact of our distinctive feature-based adaptive segmentation versus conventional fixed-rate processing. The encoder employs strided convolutional layers for downsampling, bidirectional LSTM layers for temporal modeling, and residual blocks for feature refinement. The decoder mirrors this structure to reconstruct the waveform from the latent representation.
<doc2xLabelTranslationLabelBlock>
我们使用广泛采用的基于SEANet [30]的编码器 - 解码器架构来实现我们的独特编解码器，该架构已成为现代神经语音编解码器（包括SoundStream [15]、EnCodec [59]、DAC [59]和SpeechTokenizer [16]）的事实上的标准。这种架构选择使得能够与现有方法进行直接比较，同时隔离基于独特特征的自适应分割与传统固定速率处理的影响。编码器采用步长卷积层进行下采样，双向LSTM层进行时间建模，以及残差块进行特征细化。解码器镜像此结构以从潜在表示重建波形。
</doc2xLabelTranslationLabelBlock>


For training and evaluation, we use the LibriSpeech dataset [60], a widely-adopted benchmark containing 960 hours of read English speech with diverse speakers and acoustic conditions. We train on the standard train set and evaluate codec reconstruction quality on 500 randomly selected samples from the test-clean set. This evaluation protocol maintains consistency with prior codec research and enables direct performance comparison across methods. For downstream evaluation of depression detection, we use the DAIC-WOZ dataset [61], which contains clinical interviews specifically designed for mental health assessment and provides naturalistic speech with diagnostically relevant temporal dynamics.

为了进行训练和评估，我们使用==LibriSpeech数据集==[60]，这是一个广泛采用的基准，包含960小时不同说话者和声学条件的英语朗读语音。我们在标准训练集上进行训练，并在从测试清洁集中随机选择的500个样本上评估编解码器的重建质量。这种评估协议与先前的编解码器研究保持一致，并能够直接比较不同方法的性能。对于抑郁症检测的下游评估，我们使用DAIC - WOZ数据集[61]，该数据集包含专门为心理健康评估设计的临床访谈，并提供具有诊断相关时间动态的自然语音。



<!-- Media -->



Algorithm 1 Group-wise Scalar Quantization (GSQ)
<doc2xLabelTranslationLabelBlock>
算法1 分组标量量化（GSQ）
</doc2xLabelTranslationLabelBlock>


---



Require: Input tensor \( x \in  {\mathbb{R}}^{B \times  T \times  D} \)
<doc2xLabelTranslationLabelBlock>
要求：输入张量\( x \in  {\mathbb{R}}^{B \times  T \times  D} \)
</doc2xLabelTranslationLabelBlock>

Require: Number of groups \( G \) ,group size \( d = D/G \)
<doc2xLabelTranslationLabelBlock>
要求：组数\( G \)，组大小\( d = D/G \)
</doc2xLabelTranslationLabelBlock>

Require: Use projection: use_proj \( \in  \{ \) true,false \( \} \)
<doc2xLabelTranslationLabelBlock>
要求：使用投影：use_proj \( \in  \{ \) true, false \( \} \)
</doc2xLabelTranslationLabelBlock>

Require: Projection dimension: \( k \) (typically \( k = 1 \) for M2O)
<doc2xLabelTranslationLabelBlock>
要求：投影维度：\( k \)（对于M2O通常为\( k = 1 \)）
</doc2xLabelTranslationLabelBlock>

Require: If use_proj: compression matrices \( \left\{  {{W}_{i}^{\left( c\right) } \in  }\right. \)
<doc2xLabelTranslationLabelBlock>
要求：如果使用投影：压缩矩阵\( \left\{  {{W}_{i}^{\left( c\right) } \in  }\right. \)
</doc2xLabelTranslationLabelBlock>

		\( {\left. {\mathbb{R}}^{k \times  d}\right\}  }_{i = 1}^{G} \) ,expansion matrices \( {\left\{  {W}_{i}^{\left( e\right) } \in  {\mathbb{R}}^{d \times  k}\right\}  }_{i = 1}^{G} \)
<doc2xLabelTranslationLabelBlock>
		\( {\left. {\mathbb{R}}^{k \times  d}\right\}  }_{i = 1}^{G} \)，扩展矩阵\( {\left\{  {W}_{i}^{\left( e\right) } \in  {\mathbb{R}}^{d \times  k}\right\}  }_{i = 1}^{G} \)
</doc2xLabelTranslationLabelBlock>

Require: \( \mathrm{{FSQ}} \) quantizers \( {\left\{  {\mathrm{{FSQ}}}_{i}\right\}  }_{i = 1}^{G} \)
<doc2xLabelTranslationLabelBlock>
要求：\( \mathrm{{FSQ}} \)个量化器\( {\left\{  {\mathrm{{FSQ}}}_{i}\right\}  }_{i = 1}^{G} \)
</doc2xLabelTranslationLabelBlock>

Ensure: Quantized output \( \widehat{x} \in  {\mathbb{R}}^{B \times  T \times  D} \)
<doc2xLabelTranslationLabelBlock>
确保：量化输出\( \widehat{x} \in  {\mathbb{R}}^{B \times  T \times  D} \)
</doc2xLabelTranslationLabelBlock>

		for \( i = 1 \) to \( G \) do
<doc2xLabelTranslationLabelBlock>
		对于\( i = 1 \)到\( G \)执行
</doc2xLabelTranslationLabelBlock>

			\( {x}_{i} \leftarrow  x\left\lbrack  { : , : ,i \cdot  d : \left( {i + 1}\right)  \cdot  d}\right\rbrack  \{ \) Extract group \( i\} \)
<doc2xLabelTranslationLabelBlock>
			\( {x}_{i} \leftarrow  x\left\lbrack  { : , : ,i \cdot  d : \left( {i + 1}\right)  \cdot  d}\right\rbrack  \{ \)提取组\( i\} \)
</doc2xLabelTranslationLabelBlock>

			if use_proj then \( \{ \mathrm{M}2\mathrm{O} \) : project to lower dim \( \} \)
<doc2xLabelTranslationLabelBlock>
			如果使用投影，则\( \{ \mathrm{M}2\mathrm{O} \)：投影到更低维度\( \} \)
</doc2xLabelTranslationLabelBlock>

				\( {x}_{i} \leftarrow  {W}_{i}^{\left( c\right) }{x}_{i} \)


			end if
<doc2xLabelTranslationLabelBlock>
			结束条件判断
</doc2xLabelTranslationLabelBlock>

			\( {\widehat{x}}_{i} \leftarrow  {\mathrm{{FSQ}}}_{i}\left( {x}_{i}\right) \{ \) Quantize (M2M or M2O) \( \} \)
<doc2xLabelTranslationLabelBlock>
			\( {\widehat{x}}_{i} \leftarrow  {\mathrm{{FSQ}}}_{i}\left( {x}_{i}\right) \{ \)量化（M2M或M2O）\( \} \)
</doc2xLabelTranslationLabelBlock>

			if use_proj then \( \{ \mathrm{M}2\mathrm{O} \) : project back \( \} \)
<doc2xLabelTranslationLabelBlock>
			如果使用投影，则\( \{ \mathrm{M}2\mathrm{O} \)：投影回原来维度\( \} \)
</doc2xLabelTranslationLabelBlock>

				\( {\widehat{x}}_{i} \leftarrow  {W}_{i}^{\left( e\right) }{\widehat{x}}_{i} \)


			end if
<doc2xLabelTranslationLabelBlock>
			结束条件判断
</doc2xLabelTranslationLabelBlock>

		end for
<doc2xLabelTranslationLabelBlock>
		结束循环
</doc2xLabelTranslationLabelBlock>

		\( \widehat{x} \leftarrow  \operatorname{Concat}\left( {\left\lbrack  {{\widehat{x}}_{1},{\widehat{x}}_{2},\ldots ,{\widehat{x}}_{G}}\right\rbrack  ,\dim  =  - 1}\right) \)


		return \( \widehat{x} \)
<doc2xLabelTranslationLabelBlock>
		返回\( \widehat{x} \)
</doc2xLabelTranslationLabelBlock>


---



<!-- Media -->



## A. Details of Distinctive Feature Detector
<doc2xLabelTranslationLabelBlock>
## A. 独特特征检测器的细节
</doc2xLabelTranslationLabelBlock>


The Distinctive Feature Detector, a core component of our framework, was implemented as a lightweight CNN-based architecture designed to identify perceptually significant transitions in speech signals using contrastive learning.
<doc2xLabelTranslationLabelBlock>
独特特征检测器是我们框架的核心组件，它被实现为一个基于轻量级卷积神经网络的架构，旨在通过对比学习识别语音信号中在感知上显著的过渡。
</doc2xLabelTranslationLabelBlock>


a) Model Architecture: The detector uses a 5-layer CNN structure processing raw audio input directly. Each layer consists of a 1D convolutional operation followed by batch normalization and LeakyReLU activation. The network employs variable kernel sizes and strides to progressively downsample the input while capturing acoustic patterns at different time scales. For our primary configuration which yields 9.5 tokens per second (as shown in Table II), the convolutional layers use kernel sizes of10,8,8,4,and 4,with corresponding stride values of5,4,4,2,and 2 . For our higher frame rate configuration (15.7 tokens per second), we adjusted the stride values to 5, 4, 4, 4, and 2, demonstrating the adaptability of our approach. This flexibility allows our distinctive feature-based method to operate effectively across different token rates, unlike approaches such as Sylber Codec that are limited to specific syllable-level rates. Our configurable architecture provides an effective receptive field capable of capturing both local and broader acoustic transitions while allowing token rate adjustments based on application requirements.
<doc2xLabelTranslationLabelBlock>
a) 模型架构：该检测器使用5层卷积神经网络（CNN）结构直接处理原始音频输入。每一层都由一维卷积操作、批量归一化和LeakyReLU激活组成。网络采用可变的内核大小和步长，在捕获不同时间尺度的声学模式时逐步对输入进行下采样。对于我们每秒产生9.5个令牌的主要配置（如表II所示），卷积层使用的内核大小为10、8、8、4和4，对应的步长值为5、4、4、2和2。对于我们更高帧率的配置（每秒15.7个令牌），我们将步长值调整为5、4、4、4和2，这展示了我们方法的适应性。这种灵活性使我们基于独特特征的方法能够在不同的令牌率下有效运行，这与诸如Sylber Codec等限于特定音节级速率的方法不同。我们可配置的架构提供了一个有效的感受野，能够捕获局部和更广泛的声学过渡，同时允许根据应用需求调整令牌率。
</doc2xLabelTranslationLabelBlock>


The network’s final embedding dimension was set to 256 , with an optional projection layer that could further reduce this dimension to 64 for more compact representations. We found that applying this projection with a linear transformation worked well in practice, so we set z_proj_linear to true in our experiments. To enhance the model's robustness and prevent overfitting, we incorporated an optional dropout mechanism in the projection layers, though we found that for our primary experiments, setting the dropout rate to 0 yielded optimal results.
<doc2xLabelTranslationLabelBlock>
网络的最终嵌入维度设置为256，有一个可选的投影层，可将此维度进一步减少到64以获得更紧凑的表示。我们发现在实践中应用这种线性变换的投影效果很好，所以我们在实验中将z_proj_linear设置为true。为了增强模型的鲁棒性并防止过拟合，我们在投影层中加入了一个可选的随机失活机制，不过我们发现在我们的主要实验中，将随机失活率设置为0能产生最优结果。
</doc2xLabelTranslationLabelBlock>


b) Contrastive Learning Approach: For training, we employed a contrastive learning objective where the model learned to identify acoustic boundaries by predicting future frames. We used a single-step prediction horizon (pred_steps=1) with no offset (pred_offset \( = 0 \) ),which we found provided the most reliable boundary detection performance. The similarity between predicted and actual frames was measured using cosine similarity with a scaling coefficient of 1.0 .

b) ==对比学习方法==：在训练中，我们采用了对比学习目标，模型通过预测未来帧来学习识别声学边界。我们使用单步预测范围（pred_steps=1）且无偏移（pred_offset \( = 0 \) ），我们发现这能提供最可靠的边界检测性能。使用缩放系数为1.0的余弦相似度来测量预测帧与实际帧之间的相似度。



For each positive pair, we constructed a negative pair using a random permutation strategy. While our implementation supported both within-utterance and cross-utterance negative samples through the batch_shuffle parameter, we found that using within-utterance negatives (setting batch_shuffle=false) produced more consistent results, as it forced the model to learn fine-grained distinctions within the same acoustic context.
<doc2xLabelTranslationLabelBlock>
对于每一个正样本对，我们使用随机排列策略构建一个负样本对。虽然我们的实现通过batch_shuffle参数支持话语内和跨话语的负样本，但我们发现使用话语内负样本（设置batch_shuffle=false）能产生更一致的结果，因为这迫使模型在相同的声学环境中学习细粒度的差异。
</doc2xLabelTranslationLabelBlock>


c) Training Details: The detector was trained on a speech dataset, with the primary experiments conducted using the Liberspeech dataset. We used the Adam optimizer with a learning rate of 0.0002 , a batch size of 80 , and trained for up to 200 epochs. Early stopping was employed based on validation performance to prevent overfitting. All training was conducted on NVIDIA V100 GPUs.
<doc2xLabelTranslationLabelBlock>
c) 训练细节：检测器在一个语音数据集上进行训练，主要实验使用Liberspeech数据集。我们使用Adam优化器，学习率为0.0002，批量大小为80，并训练多达200个轮次。基于验证性能采用早期停止来防止过拟合。所有训练都在NVIDIA V100 GPU上进行。
</doc2xLabelTranslationLabelBlock>


d) Boundary Detection Inference: During inference, the feature detector outputs similarity scores that undergo several post-processing steps to identify boundaries. The raw scores from prediction steps are combined and normalized using min-max normalization. Boundary detection is then performed using a peak detection algorithm, which identifies local maxima in the processed similarity scores. The key parameters for peak detection include a prominence threshold of 0.01 , along with optional width and distance constraints that were automatically tuned during training.
<doc2xLabelTranslationLabelBlock>
d) 边界检测推理：在推理过程中，特征检测器输出相似度分数，这些分数经过几个后处理步骤来识别边界。预测步骤的原始分数使用最小 - 最大归一化进行组合和归一化。然后使用峰值检测算法进行边界检测，该算法在处理后的相似度分数中识别局部最大值。峰值检测的关键参数包括突出阈值0.01，以及在训练期间自动调整的可选宽度和距离约束。
</doc2xLabelTranslationLabelBlock>


## B. Details of Distinctive Codec
<doc2xLabelTranslationLabelBlock>
## B. 独特编解码器的细节
</doc2xLabelTranslationLabelBlock>


The Distinctive Codec builds upon the SEANet-based encoder-decoder architecture used in SpeechTokenizer, extending it with our distinctive feature detection and variable-length segment processing capabilities. Here, we provide implementation details not covered in the main text.

**独特编解码器基于SpeechTokenizer中使用的基于SEANet的编码器 - 解码器架构构建，并通过我们独特的特征检测和可变长度段处理能力对其进行扩展**。在此，我们提供正文中未涵盖的实现细节。



a) Model Architecture: The encoder consists of a SEANe-tEncoder with 64 initial filters and a feature dimension of 1024. For our primary configuration yielding 9.5 tokens per second, we use strides of \( \left\lbrack  {8,5,4,2}\right\rbrack \) (resulting in a total downsampling ratio of 320 ). For our higher frame rate configuration (15.7 tokens per second),we employ strides of \( \left\lbrack  {8,5,2,2}\right\rbrack \) ,which produces a lower downsampling ratio and consequently more tokens per second. The encoder includes 2 bidirectional LSTM layers and a residual network with kernel size 3 and 1 residual layer per block. After encoding, the high-dimensional features (1024) are projected to a lower dimension (72) to make the subsequent distinctive feature processing more efficient.
<doc2xLabelTranslationLabelBlock>
a) 模型架构：编码器由一个具有64个初始滤波器和1024特征维度的SEANe - tEncoder组成。对于我们每秒产生9.5个令牌的主要配置，我们使用 \( \left\lbrack  {8,5,4,2}\right\rbrack \) 的步长（导致总下采样率为320）。对于我们更高帧率的配置（每秒15.7个令牌），我们采用 \( \left\lbrack  {8,5,2,2}\right\rbrack \) 的步长，这会产生较低的下采样率并因此每秒产生更多令牌。编码器包括2个双向长短期记忆（LSTM）层和一个内核大小为3且每个块有1个残差层的残差网络。编码后，高维特征（1024）被投影到较低维度（72），以使后续的独特特征处理更高效。
</doc2xLabelTranslationLabelBlock>


The Distinctive Feature Encoder (DFE), implemented as the PerSegmentAutoEncoder in our code, compresses variable-length segments into fixed-length representations. The encoder component uses two convolutional layers with kernel size 3 and stride 1 , followed by an adaptive average pooling operation to compress the temporal dimension to a single token. This architecture efficiently captures the salient information within each distinctive segment while maintaining a consistent output shape regardless of input segment length.

独特特征编码器（DFE），在我们的代码中实现为PerSegmentAutoEncoder，==将可变长度段压缩为固定长度表示==。编码器组件使用两个内核大小为3且步长为1的卷积层，随后进行自适应平均池化操作，将时间维度压缩为单个令牌。这种架构有效地捕获每个独特段内的显著信息，同时无论输入段长度如何都保持一致的输出形状。



For quantization, we implemented Group-wise Scalar Quantization (GSQ) through our RefinedProjectionFSQ module, which divides the feature vector into multiple groups. Each group undergoes independent projection-based quantization to improve stability and representation quality. This approach was crucial for maintaining performance when operating at lower token rates.
<doc2xLabelTranslationLabelBlock>
对于量化，我们通过改进的投影FSQ模块实现了分组标量量化（GSQ），该模块将特征向量划分为多个组。每个组都经过基于独立投影的量化，以提高稳定性和表示质量。这种方法对于在较低令牌率下运行时保持性能至关重要。
</doc2xLabelTranslationLabelBlock>


The Distinctive Feature Decoder (DFD), also implemented within the PerSegmentAutoEncoder module, reconstructs the variable-length segments from the quantized representations. It uses nearest-neighbor interpolation to expand the fixed-length representations to their original temporal dimensions, followed by two convolutional layers with kernel size 3 and stride 1 to refine the expanded features. The decoder output is then projected back to the original high dimension (1024) before being processed by the SEANetDecoder, which mirrors the encoder structure to generate the final waveform.
<doc2xLabelTranslationLabelBlock>
独特特征解码器（DFD）也在每段自动编码器模块中实现，它从量化表示中重建可变长度的段。它使用最近邻插值将固定长度的表示扩展到其原始时间维度，然后是两个内核大小为3、步长为1的卷积层，以细化扩展后的特征。然后，解码器输出在由SEANet解码器处理之前被投影回原始高维（1024），SEANet解码器镜像编码器结构以生成最终波形。
</doc2xLabelTranslationLabelBlock>


b) Training Methodology: The Distinctive Codec was trained using a combination of reconstruction and perceptual losses:
<doc2xLabelTranslationLabelBlock>
b）训练方法：独特编解码器使用重建损失和感知损失的组合进行训练：
</doc2xLabelTranslationLabelBlock>


- Time-Domain Reconstruction Loss: We used L1 loss between the original and reconstructed waveforms, weighted by a factor of 500 to ensure accurate time-domain reconstruction.
<doc2xLabelTranslationLabelBlock>
- 时域重建损失：我们使用原始波形和重建波形之间的L1损失，并乘以500的因子进行加权，以确保准确的时域重建。
</doc2xLabelTranslationLabelBlock>


- Multi-resolution Mel-spectrogram Loss: To capture perceptual qualities at different time scales, we employed a multi-resolution approach with four mel-spectrogram losses at different resolutions (using FFT sizes that vary by factors of 2). These losses combined L1 and L2 distances and were weighted at \( \left\lbrack  {{45},1,1,1}\right\rbrack \) respectively, emphasizing the base resolution.
<doc2xLabelTranslationLabelBlock>
- 多分辨率梅尔频谱图损失：为了在不同时间尺度上捕捉感知质量，我们采用了一种多分辨率方法，使用四个不同分辨率的梅尔频谱图损失（使用按2的因子变化的FFT大小）。这些损失结合了L1和L2距离，并分别在\( \left\lbrack  {{45},1,1,1}\right\rbrack \)处加权，强调基本分辨率。
</doc2xLabelTranslationLabelBlock>


- Adversarial Losses: We employed multiple discriminators to improve the perceptual quality:

- 对抗损失：我们使==用多个鉴别器来提高感知质量==：



- Multi-Period Discriminators with periods \( \lbrack 2,3,5,7 \) , 11]
<doc2xLabelTranslationLabelBlock>
- 周期为\( \lbrack 2,3,5,7 \)、11的多周期鉴别器
</doc2xLabelTranslationLabelBlock>


- Multi-Scale Discriminators operating at different resolutions
<doc2xLabelTranslationLabelBlock>
- 在不同分辨率下运行的多尺度鉴别器
</doc2xLabelTranslationLabelBlock>


- Multi-Scale STFT Discriminators analyzing the spectral characteristics
<doc2xLabelTranslationLabelBlock>
- 分析频谱特征的多尺度STFT鉴别器
</doc2xLabelTranslationLabelBlock>


c) Implementation Details: The model was trained for 20 epochs with a batch size of 9 using the Adam optimizer with learning rate \( 1\mathrm{e} - 4 \) and betas \( \left\lbrack  {{0.9},{0.99}}\right\rbrack \) . Training was performed on LibriSpeech using 48000-sample segments ( 3 seconds at \( {16}\mathrm{{kHz}} \) ) on \( 4\mathrm{{NVIDIA}}\mathrm{V}{100}\mathrm{{GPUs}} \) . We used a cosine annealing learning rate schedule over the course of training. To ensure stable training, we found that initializing network weights with near-orthogonal initialization improved convergence. The model checkpoints were saved every 2,500 steps, with the final model selected based on the lowest validation mel-spectrogram error. The average token rate of our model is 9.5 tokens per second, with the actual rate varying based on the acoustic complexity of the input speech.

c）实现细节：该模型使用Adam优化器，学习率为\( 1\mathrm{e} - 4 \)，β值为\( \left\lbrack  {{0.9},{0.99}}\right\rbrack \)，以9的批量大小训练20个epoch。在LibriSpeech上使用48000样本段（在\( {16}\mathrm{{kHz}} \)时为3秒）在\( 4\mathrm{{NVIDIA}}\mathrm{V}{100}\mathrm{{GPUs}} \)上进行训练。我们在训练过程中使用了余弦退火学习率调度。为确保训练稳定，我们发现使用近似正交初始化来初始化网络权重可提高收敛性。每2500步保存一次模型检查点，最终模型根据最低验证梅尔频谱图误差进行选择。我们模型的平均令牌率为每秒9.5个令牌，==实际速率根据输入语音的声学复杂度而变化==。



<!-- Media -->



TABLE II: Performance comparison of frame-based and distinctive feature-based speech tokenization methods. Frame: frame rate (Hz); TKR: tokens per second [62]; BPS: bits per second [58]. RVQ: Residual Vector Quantizer [63]; FSQ: Finite Scalar Quantization [31]; GSQ: Group-wise Scalar Quantization (ours). Lower values are better for MEL Error, STFT, and WER; higher values are better for PESQ [64] and STOI [65]. All models use identical SEANet-based encoder-decoder architecture trained on LibriSpeech.
<doc2xLabelTranslationLabelBlock>
表II：基于帧和基于独特特征的语音令牌化方法的性能比较。Frame：帧率（Hz）；TKR：每秒令牌数[62]；BPS：每秒比特数[58]。RVQ：残差向量量化器[63]；FSQ：有限标量量化[31]；GSQ：分组标量量化（我们的方法）。对于梅尔误差、STFT和WER，值越低越好；对于PESQ[64]和STOI[65]，值越高越好。所有模型都使用相同的基于SEANet的编码器 - 解码器架构，在LibriSpeech上进行训练。
</doc2xLabelTranslationLabelBlock>


<table><tr><td rowspan="2">Segmentation</td><td rowspan="2">Quantization</td><td rowspan="2">Frame</td><td rowspan="2">TKR</td><td rowspan="2">BPS</td><td colspan="5">\( \mathbf{{Metrics}} \)</td></tr><tr><td>MEL Error↓</td><td>STFT↓</td><td>PESQ↑</td><td>STOI↑</td><td>WER↓</td></tr><tr><td rowspan="5">Frame-based</td><td>RVQ</td><td>10</td><td>10</td><td>100</td><td>0.4487</td><td>2.0183</td><td>1.2844</td><td>0.6695</td><td>0.9340</td></tr><tr><td>RVQ</td><td>12.5</td><td>12.5</td><td>125</td><td>0.4290</td><td>1.9448</td><td>1.3245</td><td>0.6624</td><td>0.7701</td></tr><tr><td>RVQ</td><td>20</td><td>20</td><td>200</td><td>0.3796</td><td>1.7401</td><td>1.4960</td><td>0.7225</td><td>0.6887</td></tr><tr><td>RVQ</td><td>50</td><td>50</td><td>500</td><td>0.2342</td><td>0.5648</td><td>2.4496</td><td>0.8439</td><td>0.1698</td></tr><tr><td>FSQ</td><td>20</td><td>20</td><td>320</td><td>0.1933</td><td>0.4699</td><td>2.5891</td><td>0.8615</td><td>0.1428</td></tr><tr><td rowspan="4">Distinctive Feature -based (Ours)</td><td>RVQ</td><td>9.5</td><td>9.5</td><td>95</td><td>0.4481</td><td>2.0040</td><td>1.3312</td><td>0.6930</td><td>0.8523</td></tr><tr><td>FSQ</td><td>9.5</td><td>9.5</td><td>152</td><td>0.4033</td><td>1.9042</td><td>1.4649</td><td>0.7049</td><td>0.6794</td></tr><tr><td>GSQ</td><td>9.5</td><td>9.5</td><td>152</td><td>0.2857</td><td>1.3213</td><td>1.9147</td><td>0.7675</td><td>0.4265</td></tr><tr><td>GSQ</td><td>15.7</td><td>15.7</td><td>251</td><td>0.2468</td><td>0.9072</td><td>2.3092</td><td>0.8203</td><td>0.2637</td></tr></table>
<doc2xLabelTranslationLabelBlock>
<table><tbody><tr><td rowspan="2">分割</td><td rowspan="2">量化</td><td rowspan="2">帧</td><td rowspan="2">TKR</td><td rowspan="2">BPS</td><td colspan="5">\( \mathbf{{Metrics}} \)</td></tr><tr><td>MEL误差↓</td><td>STFT↓</td><td>PESQ↑</td><td>STOI↑</td><td>WER↓</td></tr><tr><td rowspan="5">基于帧的</td><td>RVQ</td><td>10</td><td>10</td><td>100</td><td>0.4487</td><td>2.0183</td><td>1.2844</td><td>0.6695</td><td>0.9340</td></tr><tr><td>RVQ</td><td>12.5</td><td>12.5</td><td>125</td><td>0.4290</td><td>1.9448</td><td>1.3245</td><td>0.6624</td><td>0.7701</td></tr><tr><td>RVQ</td><td>20</td><td>20</td><td>200</td><td>0.3796</td><td>1.7401</td><td>1.4960</td><td>0.7225</td><td>0.6887</td></tr><tr><td>RVQ</td><td>50</td><td>50</td><td>500</td><td>0.2342</td><td>0.5648</td><td>2.4496</td><td>0.8439</td><td>0.1698</td></tr><tr><td>FSQ</td><td>20</td><td>20</td><td>320</td><td>0.1933</td><td>0.4699</td><td>2.5891</td><td>0.8615</td><td>0.1428</td></tr><tr><td rowspan="4">基于独特特征（我们的方法）</td><td>RVQ</td><td>9.5</td><td>9.5</td><td>95</td><td>0.4481</td><td>2.0040</td><td>1.3312</td><td>0.6930</td><td>0.8523</td></tr><tr><td>FSQ</td><td>9.5</td><td>9.5</td><td>152</td><td>0.4033</td><td>1.9042</td><td>1.4649</td><td>0.7049</td><td>0.6794</td></tr><tr><td>GSQ</td><td>9.5</td><td>9.5</td><td>152</td><td>0.2857</td><td>1.3213</td><td>1.9147</td><td>0.7675</td><td>0.4265</td></tr><tr><td>GSQ</td><td>15.7</td><td>15.7</td><td>251</td><td>0.2468</td><td>0.9072</td><td>2.3092</td><td>0.8203</td><td>0.2637</td></tr></tbody></table>
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



## C. Codec Evaluation Metrics
<doc2xLabelTranslationLabelBlock>
## C. 编解码器评估指标
</doc2xLabelTranslationLabelBlock>


We evaluate our Distinctive Codec using metrics for reconstruction quality, intelligibility, and encoding efficiency following previous works [15], [14]. For reconstruction, we measure mel-spectral error, STFT distance, and PESQ (Perceptual Evaluation of Speech Quality) scores [64]. For intelligibility, we measure Word Error Rate (WER) using the Whisper en-medium model [66], following SpeechTokenizer [16], and STOI (Short-Time Objective Intelligibility) [65] to quantify how accurately the speech content is preserved. We also track encoding efficiency through Token Ratio (TKR) [62], representing tokens per second of \( {16}\mathrm{{kHz}} \) audio,and Bits Per Second (BPS). The BPS calculation follows the approach in [58] which will consider the vocabulary size of quantization method.
<doc2xLabelTranslationLabelBlock>
我们按照先前的工作[15]、[14]，使用重建质量、可懂度和编码效率指标来评估我们的独特编解码器。对于重建，我们测量梅尔频谱误差、短时傅里叶变换（STFT）距离和语音质量感知评估（PESQ）分数[64]。对于可懂度，我们使用Whisper en-medium模型[66]，按照语音分词器[16]的方法测量单词错误率（WER），并使用短时目标可懂度（STOI）[65]来量化语音内容保留的准确程度。我们还通过令牌比率（TKR）[62]来跟踪编码效率，TKR表示\( {16}\mathrm{{kHz}} \)音频每秒的令牌数，以及每秒比特数（BPS）。BPS的计算遵循[58]中的方法，该方法将考虑量化方法的词汇量大小。
</doc2xLabelTranslationLabelBlock>


## D. Depression Detection Evaluation
<doc2xLabelTranslationLabelBlock>
## D. 抑郁症检测评估
</doc2xLabelTranslationLabelBlock>


Evaluating the downstream performance of speech tokenizers for clinical applications presents fundamental methodological challenges. Training full-scale clinical assessment models requires substantial computational resources and carefully curated clinical data, making comprehensive comparison across multiple tokenization approaches prohibitively expensive. Furthermore, traditional codec evaluation metrics that focus on reconstruction quality and speech recognition fail to capture the preservation of subtle acoustic and temporal characteristics essential for mental health assessment, creating a significant evaluation gap between codec performance and clinical utility.
<doc2xLabelTranslationLabelBlock>
评估用于临床应用的语音分词器的下游性能存在基本的方法学挑战。训练全面的临床评估模型需要大量的计算资源和精心策划的临床数据，使得跨多种分词方法进行全面比较成本过高。此外，专注于重建质量和语音识别的传统编解码器评估指标无法捕捉到心理健康评估所需的微妙声学和时间特征的保留情况，在编解码器性能和临床效用之间造成了显著的评估差距。
</doc2xLabelTranslationLabelBlock>


To enable systematic comparison of how different tokeniza-tion strategies preserve clinically relevant temporal information, we adopt the token projection evaluation framework proposed in [67]. This methodology isolates the effects of tokenization by maintaining identical downstream architectures and training procedures across all evaluated methods, ensuring that performance differences directly reflect the quality of information preserved in discrete representations rather than variations in model design. We apply this comparative framework to the binary depression classification task using the DAIC-WOZ dataset [61], which requires capturing subtle prosodic variations, pause patterns, and speech rate fluctuations-the temporal dynamics that our distinctive feature-based approach is designed to preserve.

为了能够系统地比较不同分词策略如何保留与临床相关的时间信息，我们采用了[67]中提出的==令牌投影评估框架==。这种方法通过在所有评估方法中保持相同的下游架构和训练过程来隔离分词的影响，确保性能差异直接反映离散表示中保留的信息质量，而不是模型设计的变化。我们将这个比较框架应用于使用==DAIC-WOZ数据集==[61]的二元抑郁症分类任务，该任务==需要捕捉微妙的韵律变化、停顿模式和语速波动==——我们基于独特特征的方法旨在保留的时间动态。



a) Evaluation Protocol.: Depression detection is evaluated on the DAIC-WOZ dataset, containing 107 training participants and 35 development set participants from clinical interviews. Following standard protocols, we extract participant speech segments (excluding interviewer turns) from each session and tokenize them using different codec methods under comparison. Depression labels are derived from PHQ-8 scores, with scores \( \geq  {10} \) indicating clinical depression. For each tokenization method, we employ a Llama 3.1 8B model [2] with trainable projection module and classification components, trained using AdamW (lr=5 \( \times  {10}^{-5} \) ,batch size 16) for 50 epochs with bf16 mixed precision. Participant speech from each session is processed as variable-length segments projected to 128 tokens. We report F1-score, UAR, and accuracy as evaluation metrics, with all experiments conducted three times using different random seeds and results averaged across runs.

a) 评估协议：在DAIC-WOZ数据集上评估抑郁症检测，该数据集包含来自临床访谈的107名训练参与者和35名开发集参与者。按照标准协议，我们从每个会话中提取参与者语音片段（不包括访谈者的轮次），并使用正在比较的不同编解码器方法对其进行分词。抑郁症标签来自PHQ-8分数，分数\( \geq  {10} \)表示临床抑郁症。对于每种分词方法，我们使用带有可训练投影模块和分类组件的Llama 3.1 8B模型[2]，使用AdamW（学习率=5 \( \times  {10}^{-5} \) ，批量大小16）进行50个轮次的训练，采用bf16混合精度。每个会话的参与者语音作为投影到128个令牌的可变长度片段进行处理。我们报告F1分数、UAR和准确率作为评估指标，==所有实验使用不同的随机种子进行三次，并对运行结果进行平均==。



## V. EXPERIMENTAL RESULTS
<doc2xLabelTranslationLabelBlock>
## V. 实验结果
</doc2xLabelTranslationLabelBlock>


## A. Codec Results
<doc2xLabelTranslationLabelBlock>
## A. 编解码器结果
</doc2xLabelTranslationLabelBlock>


Table II presents a systematic comparison between frame-based and distinctive feature-based tokenization approaches under identical architectural and training conditions. Our distinctive feature-based codec operates at average token rates of \( {9.5}\mathrm{\;{Hz}} \) and \( {15.7}\mathrm{\;{Hz}} \) ,significantly lower than conventional fixed-rate processing while maintaining superior reconstruction quality.

表二展示了在相同架构和训练条件下基于帧和基于独特特征的分词方法之间的系统比较。我们基于独特特征的编解码器以\( {9.5}\mathrm{\;{Hz}} \)和\( {15.7}\mathrm{\;{Hz}} \)的平均令牌率运行，==明显低于传统固定速率处理，同时保持了卓越的重建质量==。



<!-- Media -->



TABLE III: Depression detection performance comparison on DAIC-WOZ development set. All models use identical downstream architecture (Llama 3.18B with projection module) and training protocol. Results are averaged over three random seeds.
<doc2xLabelTranslationLabelBlock>
表三：DAIC-WOZ开发集上的抑郁症检测性能比较。所有模型使用相同的下游架构（带有投影模块的Llama 3.18B）和训练协议。结果是在三个随机种子上平均得到的。
</doc2xLabelTranslationLabelBlock>


<table><tr><td>Tokenization Method</td><td>F1-Score</td></tr><tr><td>Frame-based (10Hz RVQ)</td><td>0.471</td></tr><tr><td>Distinctive Feature-based ( \( {9.5}\mathrm{{Hz}} \) RVQ)</td><td>0.533</td></tr><tr><td>Distinctive Feature-based ( \( {9.5}\mathrm{{Hz}} \) GSQ)</td><td>0.636</td></tr></table>
<doc2xLabelTranslationLabelBlock>
<table><tbody><tr><td>分词方法</td><td>F1分数</td></tr><tr><td>基于帧的(10Hz RVQ)</td><td>0.471</td></tr><tr><td>基于特征的(\( {9.5}\mathrm{{Hz}} \) RVQ)</td><td>0.533</td></tr><tr><td>基于特征的(\( {9.5}\mathrm{{Hz}} \) GSQ)</td><td>0.636</td></tr></tbody></table>
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



The effectiveness of our distinctive feature-based approach is evident when comparing models at similar token rates. At \( {9.5}\mathrm{\;{Hz}} \) ,our method with RVQ quantization outperforms the frame-based baseline at \( {10}\mathrm{\;{Hz}} \) across all metrics. Despite using a slightly lower token rate, our approach reduces MEL Error by 0.0006 , decreases STFT distortion by 0.0143 , improves perceptual quality (PESQ) by 0.0468, enhances intelligibility (STOI) by 0.0235 , and reduces WER by 0.0817 . These consistent improvements demonstrate that adaptive segmentation guided by acoustic boundaries preserves more acoustic information than arbitrary fixed-interval processing, even when using fewer tokens per second.

当以相似的令牌率比较模型时，我们独特的基于特征的方法的有效性显而易见。在\( {9.5}\mathrm{\;{Hz}} \)，我们采用RVQ量化的方法在所有指标上均优于\( {10}\mathrm{\;{Hz}} \)处基于帧的基线。尽管使用的令牌率略低，但我们的方法将MEL误差降低了0.0006，将STFT失真降低了0.0143，将感知质量（PESQ）提高了0.0468，将可懂度（STOI）提高了0.0235，并将WER降低了0.0817。这些持续的改进表明，==由声学边界引导的自适应分割比任意固定间隔处理保留了更多的声学信息，即使每秒使用的令牌更少==。



We observe further improvements when replacing RVQ with FSQ in our distinctive feature-based codec. Notably, frame-based approaches exhibit instability with FSQ at lower token rates (below \( {20}\mathrm{\;{Hz}} \) ),preventing direct comparison at our model’s operating point of \( {9.5}\mathrm{\;{Hz}} \) . This instability supports our hypothesis that fixed-rate segmentation at long intervals forces the quantizer to represent acoustically heterogeneous content within single frames, leading to training difficulties. The comparison between GSQ and FSQ at the same token rate(9.5Hz)clearly demonstrates the effectiveness of our Group-wise Scalar Quantization approach. GSQ substantially outperforms FSQ across all metrics, reducing MEL error by 0.1176 , decreasing STFT distortion by 0.5829 , improving PESQ by 0.4498 , increasing STOI by 0.0626, and lowering WER by 0.2529 . This validates our hypothesis that decomposing high-dimensional quantization into multiple lower-dimensional sub-problems enhances stability and representation quality for variable-length segments generated by distinctive feature-based segmentation.

当在我们基于独特特征的编解码器中用FSQ替换RVQ时，我们观察到了进一步的改进。值得注意的是，基于帧的方法在较低令牌率（低于\( {20}\mathrm{\;{Hz}} \)）下使用FSQ时表现出不稳定性，这使得无法在我们模型的\( {9.5}\mathrm{\;{Hz}} \)操作点进行直接比较。这种不稳定性支持了我们的假设，即==长间隔的固定速率分割迫使量化器在单个帧内表示声学上异质的内容，从而导致训练困难==。在相同令牌率（9.5Hz）下GSQ和FSQ的比较清楚地证明了我们的分组标量量化方法的有效性。GSQ在所有指标上均大幅优于FSQ，将MEL误差降低了0.1176，将STFT失真降低了0.5829，将PESQ提高了0.4498，将STOI提高了0.0626，并将WER降低了0.2529。这验证了我们的假设，即==将高维量化分解为多个低维子问题可提高基于独特特征分割生成的可变长度段的稳定性和表示质量==。



## B. Depression Detection Results
<doc2xLabelTranslationLabelBlock>
## B. 抑郁症检测结果
</doc2xLabelTranslationLabelBlock>


Table III presents the depression detection performance comparison across tokenization approaches. Our distinctive feature-based codec with GSQ achieves an F1-score of 0.636, substantially outperforming both the frame-based baseline (F1=0.471, +35.0% relative improvement) and our method with standard RVQ quantization \( (\mathrm{F}1 = {0.533}, + {19.3}\% \) relative improvement). These results provide direct evidence that timing-preserving tokenization captures clinically relevant temporal dynamics that are destroyed by fixed-rate processing.
<doc2xLabelTranslationLabelBlock>
表III展示了不同令牌化方法的抑郁症检测性能比较。我们采用GSQ的基于独特特征的编解码器的F1分数为0.636，大幅优于基于帧的基线（F1 = 0.471，相对提高35.0%）和我们采用标准RVQ量化的方法\( (\mathrm{F}1 = {0.533}, + {19.3}\% \)相对提高）。这些结果提供了直接证据，表明保留时间的令牌化捕捉到了被固定速率处理破坏的临床相关时间动态。
</doc2xLabelTranslationLabelBlock>


The performance hierarchy reveals two complementary effects. First, comparing frame-based (F1=0.471) versus distinctive feature-based with RVQ (F1=0.533) isolates the contribution of adaptive segmentation: aligning token boundaries with acoustic transitions preserves the pause patterns, speech rate variations, and prosodic contours that serve as established biomarkers for depression. Second, the further gain from GSQ (F1=0.636) demonstrates that stable quantization of variable-length segments is crucial for preserving fine-grained temporal information at low token rates. Together, these findings validate that our approach-adaptive segmentation plus robust quantization-effectively encodes the temporal dynamics essential for clinical speech analysis.

性能层次揭示了两个互补效应。首先,比较基于帧的（F1 = 0.471）与基于独特特征且采用RVQ的（F1 = 0.533），分离出了自适应分割的贡献：==将令牌边界与声学过渡对齐保留了作为抑郁症既定生物标志物的停顿模式、语速变化和韵律轮廓==。其次，GSQ（F1 = 0.636）的进一步提升表明，==可变长度段的稳定量化对于在低令牌率下保留细粒度时间信息至关重要==。总之，这些发现验证了我们的方法——自适应分割加鲁棒量化——有效地编码了临床语音分析所需的时间动态。



## VI. ABLATION STUDY AND DISCUSSION
<doc2xLabelTranslationLabelBlock>
## VI. 消融研究与讨论
</doc2xLabelTranslationLabelBlock>


## A. Analysis of Distinctive Feature Effectiveness
<doc2xLabelTranslationLabelBlock>
## A. 独特特征有效性分析
</doc2xLabelTranslationLabelBlock>


a) Theoretical Derivation Hypothesis: Consider an au-toencoder that extracts a latent representation from input speech frames. For each segment of input frames \( x \in  {\mathbb{R}}^{D \times  L} \) , the encoder produces a latent vector \( z = f\left( x\right)  \in  {\mathbb{R}}^{d} \) . The autoencoder training objective and quantization distortion can be formulated as:
<doc2xLabelTranslationLabelBlock>
a) 理论推导假设：考虑一个从输入语音帧中提取潜在表示的自动编码器。对于输入帧\( x \in  {\mathbb{R}}^{D \times  L} \)的每个段，编码器产生一个潜在向量\( z = f\left( x\right)  \in  {\mathbb{R}}^{d} \)。自动编码器的训练目标和量化失真可以表述为：
</doc2xLabelTranslationLabelBlock>


\[\mathop{\min }\limits_{{f,g}}{\mathbb{E}}_{x}\left\lbrack  {\parallel x - g\left( {f\left( x\right) }\right) {\parallel }^{2}}\right\rbrack  ,\;D = \mathbb{E}\left\lbrack  {\parallel z - Q\left( z\right) {\parallel }^{2}}\right\rbrack   \tag{13}\]



where \( g \) is the decoder and \( Q \) is the quantizer that maps \( z \) to a finite codebook. High-resolution quantization theory [68] approximates this distortion as:
<doc2xLabelTranslationLabelBlock>
其中\( g \)是解码器，\( Q \)是将\( z \)映射到有限码本的量化器。高分辨率量化理论[68]将这种失真近似为：
</doc2xLabelTranslationLabelBlock>


\[D \approx  G\left( d\right) {\left( \int p{\left( z\right) }^{\frac{d}{d + 2}}dz\right) }^{\frac{d + 2}{d}}{2}^{-\frac{2R}{d}} \tag{14}\]



where \( G\left( d\right) \) is a dimension-dependent constant and \( R \) is the bit rate. The integral term is critical, as it depends on the distribution of latent vectors.
<doc2xLabelTranslationLabelBlock>
其中\( G\left( d\right) \)是一个与维度相关的常数，\( R \)是比特率。积分项很关键，因为它取决于潜在向量的分布。
</doc2xLabelTranslationLabelBlock>


Based on these formulations, we hypothesize that the effectiveness of distinctive features stems from their impact on the latent distribution \( p\left( z\right) \) . We posit that frame-based segmentation, which arbitrarily divides speech without regard to acoustic boundaries, may potentially force a single segment to capture multiple distinct speech states. This would cause the resulting latent vectors to represent a mixture of acoustic features, leading to a more diffuse distribution in the latent space. Under this hypothesis, our distinctive feature approach should yield more concentrated latent distributions by aligning segment boundaries with natural acoustic transitions. Specifically, when segments contain acoustically homogeneous content, the encoder can produce latent vectors that cluster more tightly around prototype representations of discrete speech units. Such a multimodal distribution would theoretically allow for more efficient quantization, as codebook entries could be optimally positioned to capture these distinct modes, thereby reducing the overall distortion \( D \) for a given rate \( R \) .
<doc2xLabelTranslationLabelBlock>
基于这些公式，我们假设独特特征的有效性源于它们对潜在分布\( p\left( z\right) \)的影响。我们认为，基于帧的分割，即不考虑声学边界任意划分语音，可能会迫使单个片段捕获多个不同的语音状态。这将导致生成的潜在向量代表声学特征的混合，从而在潜在空间中产生更分散的分布。在这个假设下，我们的独特特征方法应该通过将片段边界与自然声学过渡对齐，产生更集中的潜在分布。具体来说，当片段包含声学上均匀的内容时，编码器可以生成更紧密地聚集在离散语音单元原型表示周围的潜在向量。从理论上讲，这样的多模态分布将允许更有效的量化，因为码本条目可以被最佳定位以捕获这些不同的模式，从而在给定速率\( R \)下降低整体失真\( D \)。
</doc2xLabelTranslationLabelBlock>


b) Codebook Analysis: Building on our theoretical analysis, we now examine the empirical evidence supporting our hypothesis through codebook utilization patterns. Since our theoretical framework suggests that distinctive features should allow for more efficient quantization by producing more concentrated latent distributions, analyzing codebook utilization provides a direct way to verify this effect in practice.
<doc2xLabelTranslationLabelBlock>
b) 码本分析：基于我们的理论分析，我们现在通过码本使用模式来检验支持我们假设的经验证据。由于我们的理论框架表明独特特征应该通过产生更集中的潜在分布来实现更有效的量化，分析码本使用情况提供了一种在实践中验证这种效果的直接方法。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



<!-- figureText: \( {20}\mathrm{\;{Hz}} \) (Utilization: 10.05%) Utilization rate: 4.77% Usage Code Value (b) Visualization of Codebook Usage for Codec Using Distinctive Feature Frequency Ratio (%) \( {12.5}\mathrm{\;{Hz}} \) (Utilization: 2.16%) \( {10}\mathrm{\;{Hz}} \) (Utilization: 1.53%) distinctive feature (Utilization: 4.77%) 0 20 40 Rank (a) Top 50 Code Frequency Ratios with Their Overall Utilization Rates Shown in the Legend. -->



<img src="https://cdn.noedgeai.com/bo_d5ivs477aajc7383hm2g_8.jpg?x=135&y=161&w=1500&h=447&r=0"/>



Fig. 4: Codebook utilization comparison between frame-based and distinctive feature-based processing. Our approach achieves more balanced and efficient codebook usage (4.77% utilization) compared to frame-based methods at similar frame rates (1.53% at \( {10}\mathrm{{Hz}} \) ).
<doc2xLabelTranslationLabelBlock>
图4：基于帧和基于独特特征的处理之间的码本使用比较。与在相似帧率（\( {10}\mathrm{{Hz}} \)时为1.53%）下的基于帧的方法相比，我们的方法实现了更平衡和高效的码本使用（4.77%的使用率）。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



To investigate this hypothesis, we conducted experiments comparing codebook utilization across different processing approaches, with all models using FSQ for quantization. Figure 4 presents the utilization statistics for frame-based models operating at different frame rates ( \( {20}\mathrm{{Hz}},{12.5}\mathrm{{Hz}} \) ,and \( {10}\mathrm{{Hz}} \) ) alongside our Distinctive Codec. The results reveal several important insights. As shown in Figure 4a, when frame rates decrease in conventional frame-based models, codebook utilization rates drop dramatically—from \( {10.05}\% \) at \( {20}\mathrm{{Hz}} \) to merely \( {1.53}\% \) at \( {10}\mathrm{{Hz}} \) . This declining utilization explains the instability we encountered when attempting to run Speech Tokenizer with FSQ at lower frame rates, as the quantizer struggles to effectively represent the diverse acoustic content when arbitrarily segmented at longer intervals.
<doc2xLabelTranslationLabelBlock>
为了研究这个假设，我们进行了实验，比较不同处理方法的码本使用情况，所有模型都使用FSQ进行量化。图4展示了在不同帧率（\( {20}\mathrm{{Hz}},{12.5}\mathrm{{Hz}} \)和\( {10}\mathrm{{Hz}} \)）下运行的基于帧的模型以及我们的独特编解码器的使用统计数据。结果揭示了几个重要的见解。如图4a所示，在传统的基于帧的模型中，当帧率降低时，码本使用率急剧下降——从\( {20}\mathrm{{Hz}} \)时的\( {10.05}\% \)降至\( {10}\mathrm{{Hz}} \)时的仅\( {1.53}\% \)。这种使用率的下降解释了我们在尝试以较低帧率使用FSQ运行语音分词器时遇到的不稳定性，因为量化器在以较长间隔进行任意分割时难以有效表示多样的声学内容。
</doc2xLabelTranslationLabelBlock>


In stark contrast, our Distinctive Codec achieves a substantially higher codebook utilization rate of \( {4.77}\% \) despite operating at a comparable frame rate(9.5Hz)to the \( {10}\mathrm{{Hz}} \) frame-based model. This represents over three times better utilization of the quantization space. Moreover, the frequency distribution in Figure 4a shows that our approach exhibits a more balanced utilization pattern across codebook entries, indicating a more effective mapping of acoustic features to the discrete representation space. Conventional frame-based approaches show highly skewed distributions with a few dominant codes and many rarely-used entries, whereas our distinctive feature-based segmentation leads to a more uniform distribution. The visualization of the actual codebook usage in Figure \( 4\mathrm{\;b} \) further illustrates how our approach better leverages the available codebook capacity through perceptually-guided segmentation.
<doc2xLabelTranslationLabelBlock>
与之形成鲜明对比的是，尽管我们的独特编解码器在与\( {10}\mathrm{{Hz}} \)基于帧的模型相当的帧率（9.5Hz）下运行，但其码本使用率却大幅提高到\( {4.77}\% \)。这表示对量化空间的利用率提高了三倍多。此外，图4a中的频率分布表明，我们的方法在整个码本条目中呈现出更平衡的使用模式，表明声学特征到离散表示空间的映射更有效。传统的基于帧的方法显示出高度偏斜的分布，有一些主导代码和许多很少使用的条目，而我们基于独特特征的分割导致更均匀的分布。图\( 4\mathrm{\;b} \)中实际码本使用情况的可视化进一步说明了我们的方法如何通过感知引导的分割更好地利用可用的码本容量。
</doc2xLabelTranslationLabelBlock>


These empirical findings strongly support our theoretical hypothesis: by aligning segment boundaries with natural acoustic transitions, distinctive feature-based processing produces more coherent latent representations that can be more efficiently quantized. The improved codebook utilization directly translates to better reconstruction quality and speech intelligibility as demonstrated in our main experimental results, validating the fundamental advantage of our approach over uniform frame-based processing.
<doc2xLabelTranslationLabelBlock>
这些实证结果有力地支持了我们的理论假设：通过将片段边界与自然声学过渡对齐，基于独特特征的处理产生更连贯的潜在表示，这些表示可以更有效地进行量化。如我们的主要实验结果所示，提高的码本利用率直接转化为更好的重建质量和语音清晰度，验证了我们的方法相对于基于均匀帧的处理的根本优势。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



TABLE IV: Comparison between FSQ and our proposed GSQ (many to many) at different reconstruction levels. Both methods are evaluated on the same Distinctive Codec architecture operating at \( {9.5}\mathrm{\;{Hz}} \) .
<doc2xLabelTranslationLabelBlock>
表IV：在不同重建水平下FSQ和我们提出的GSQ（多对多）的比较。两种方法都在以\( {9.5}\mathrm{\;{Hz}} \)运行的相同独特编解码器架构上进行评估。
</doc2xLabelTranslationLabelBlock>


<table><tr><td>\( \mathbf{{Method}} \)</td><td>Level</td><td>MEL↓</td><td>STFT↓</td><td>PESQ↑</td><td>STOI↑</td><td>WER↓</td></tr><tr><td>FSQ</td><td>32</td><td>0.1768</td><td>0.5844</td><td>2.5467</td><td>0.8645</td><td>0.1301</td></tr><tr><td>FSQ</td><td>64</td><td>0.1407</td><td>0.4330</td><td>2.7909</td><td>0.8950</td><td>0.0732</td></tr><tr><td>GSQ</td><td>24</td><td>0.2056</td><td>0.6963</td><td>2.3578</td><td>0.8474</td><td>0.1794</td></tr><tr><td>GSQ</td><td>32</td><td>0.1872</td><td>0.6207</td><td>2.4732</td><td>0.8597</td><td>0.1433</td></tr><tr><td>GSQ</td><td>64</td><td>0.1456</td><td>0.4423</td><td>2.7848</td><td>0.8889</td><td>0.0749</td></tr></table>
<doc2xLabelTranslationLabelBlock>
<table><tbody><tr><td>\( \mathbf{{Method}} \)</td><td>等级</td><td>MEL↓</td><td>STFT↓</td><td>PESQ↑</td><td>STOI↑</td><td>WER↓</td></tr><tr><td>FSQ</td><td>32</td><td>0.1768</td><td>0.5844</td><td>2.5467</td><td>0.8645</td><td>0.1301</td></tr><tr><td>FSQ</td><td>64</td><td>0.1407</td><td>0.4330</td><td>2.7909</td><td>0.8950</td><td>0.0732</td></tr><tr><td>GSQ</td><td>24</td><td>0.2056</td><td>0.6963</td><td>2.3578</td><td>0.8474</td><td>0.1794</td></tr><tr><td>GSQ</td><td>32</td><td>0.1872</td><td>0.6207</td><td>2.4732</td><td>0.8597</td><td>0.1433</td></tr><tr><td>GSQ</td><td>64</td><td>0.1456</td><td>0.4423</td><td>2.7848</td><td>0.8889</td><td>0.0749</td></tr></tbody></table>
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



B. Analysis of Group-wise Scalar Quantization Effectiveness
<doc2xLabelTranslationLabelBlock>
B. 分组标量量化有效性分析
</doc2xLabelTranslationLabelBlock>


a) Comparative Analysis of Quantization Methods: Building on our theoretical analysis of distinctive features, we next investigate the effectiveness of our quantization strategy when applied to segments of varying complexity. Table IV compares our GSQ approach with standard FSQ across different reconstruction levels. While our primary results focused on compressing variable-length segments into single tokens, this analysis examines how effectively both methods preserve information at different dimensionalities. The reconstruction level represents the latent space dimension to which each segment is compressed before quantization. Our GSQ approach offers a significant advantage by dramatically reducing the effective dictionary size required for high-quality representations. Whereas direct application of FSQ to high-dimensional features would demand an extremely large dictionary, our group-wise strategy effectively partitions the feature space into multiple specialized quantization subproblems. This approach not only enhances computational efficiency but also improves representation stability for distinctive feature-based segments. Despite using a much smaller effective dictionary, GSQ achieves performance comparable to FSQ across all metrics, particularly at higher reconstruction levels.
<doc2xLabelTranslationLabelBlock>
a) 量化方法的比较分析：基于我们对显著特征的理论分析，接下来我们研究量化策略应用于不同复杂度片段时的有效性。表四在不同重建级别上比较了我们的GSQ方法和标准FSQ。虽然我们的主要结果集中在将可变长度片段压缩为单个令牌，但此分析考察了两种方法在不同维度上保留信息的有效性。重建级别表示每个片段在量化前被压缩到的潜在空间维度。我们的GSQ方法通过大幅减少高质量表示所需的有效字典大小提供了显著优势。而将FSQ直接应用于高维特征将需要极大的字典，我们的分组策略有效地将特征空间划分为多个专门的量化子问题。这种方法不仅提高了计算效率，还提高了基于显著特征的片段的表示稳定性。尽管使用的有效字典要小得多，但GSQ在所有指标上都能达到与FSQ相当的性能，特别是在较高的重建级别上。
</doc2xLabelTranslationLabelBlock>


b) Analysis of GSQ's Effectiveness Through Latent Space Geometry: We hypothesize that our GSQ approach outperforms standard quantization methods due to two key information-theoretic advantages. First, by projecting high-dimensional features onto specialized lower-dimensional subspaces before quantization, GSQ maximizes mutual information between input and quantized output. The learned projection matrices effectively discard redundant information while preserving essential acoustic patterns, resulting in a lower KL divergence between original and quantized distributions. Second, unlike standard FSQ which quantizes each dimension independently, GSQ's group-wise approach exploits statistical dependencies in the data, effectively aligning quantization axes with the signal's intrinsic structure. This results in quantization cells that better fit the data distribution, reducing overall distortion.
<doc2xLabelTranslationLabelBlock>
b) 通过潜在空间几何分析GSQ的有效性：我们假设我们的GSQ方法由于两个关键的信息论优势而优于标准量化方法。首先，通过在量化前将高维特征投影到专门的低维子空间上，GSQ最大化了输入和量化输出之间的互信息。学习到的投影矩阵有效地丢弃了冗余信息，同时保留了基本的声学模式，导致原始分布和量化分布之间的KL散度更低。其次，与独立量化每个维度的标准FSQ不同，GSQ的分组方法利用了数据中的统计依赖性，有效地将量化轴与信号的内在结构对齐。这导致量化单元更适合数据分布，减少了整体失真。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



<!-- figureText: Cluster 1 Cluster 2 20 -10 -20 -30 -20 -10 t-SNE dimension 1 (b) GSQ latent space clustering Cluster 2 Cluster 3 10 t-SNE dimension 2 -20 -20 -10 10 30 t-SNE dimension 1 (a) FSQ latent space clustering -->



<img src="https://cdn.noedgeai.com/bo_d5ivs477aajc7383hm2g_9.jpg?x=134&y=156&w=1525&h=605&r=0"/>



Fig. 5: t-SNE visualization [69] of latent space clusters. (a) Standard FSQ shows overlapping clusters. (b) GSQ produces more distinct clusters with clearer boundaries.
<doc2xLabelTranslationLabelBlock>
图5：潜在空间聚类的t-SNE可视化[69]。(a) 标准FSQ显示重叠聚类。(b) GSQ产生边界更清晰的更明显聚类。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



Our information-theoretic analysis suggests that if GSQ preserves more structural information, this should be reflected in the geometric organization of the latent space [70], [71]. Specifically, a quantization method that maintains higher mutual information and lower distortion should produce more coherent and well-separated clusters when visualized. To test this hypothesis, we employed t-SNE visualization [69], which maps high-dimensional data to two dimensions while preserving local neighborhood relationships, making it ideal for assessing how well structural information is maintained after quantization. Figure 5 shows striking differences between standard FSQ (Figure 5a) and our GSQ approach (Figure 5b). GSQ produces significantly more coherent clusters with clearer boundaries, indicating that GSQ preserves class structure and separability in the latent space. These results directly validate our hypothesis: GSQ's decomposition strategy prevents information mixing across feature groups, enabling it to preserves more meaningful information within severe token constraints. The improved geometric organization of GSQ's latent space explains its superior reconstruction quality across all our evaluation metrics.
<doc2xLabelTranslationLabelBlock>
我们的信息论分析表明，如果GSQ保留了更多的结构信息，这应该反映在潜在空间的几何组织中[70]，[71]。具体来说，一种保持更高互信息和更低失真的量化方法在可视化时应该产生更连贯且分隔良好的聚类。为了验证这个假设，我们采用了t-SNE可视化[69]，它将高维数据映射到二维，同时保留局部邻域关系，非常适合评估量化后结构信息的保留程度。图5显示了标准FSQ（图5a）和我们的GSQ方法（图5b）之间的显著差异。GSQ产生了明显更连贯的聚类，边界更清晰，表明GSQ在潜在空间中保留了类结构和可分离性。这些结果直接验证了我们的假设：GSQ的分解策略防止了跨特征组的信息混合，使其能够在严格的令牌约束内保留更多有意义的信息。GSQ潜在空间改进的几何组织解释了其在我们所有评估指标上的卓越重建质量。
</doc2xLabelTranslationLabelBlock>


## C. Generalization to Out-of-Domain and Code-Switched Speech
<doc2xLabelTranslationLabelBlock>
## C. 对域外和语码转换语音的泛化
</doc2xLabelTranslationLabelBlock>


To evaluate the generalization capability of our model beyond English and the LibriSpeech training domain, we conduct a zero-shot inference experiment on the SEAME dataset [72], a 200-hour spontaneous Mandarin-English code-switching corpus widely used in multilingual speech research. SEAME poses three primary challenges: (i) Conversational spontaneity: frequent disfluencies (hesitations, overlaps, false starts) and colloquial prosody; (ii) Telephony distortions and environmental noise: low-bitrate channel artifacts and background sounds typical of mobile or landline settings; (iii) Cross-lingual code-switching: rapid alternation between Mandarin and English within utterances.
<doc2xLabelTranslationLabelBlock>
为了评估我们的模型在英语和LibriSpeech训练域之外的泛化能力，我们在SEAME数据集[72]上进行了零样本推理实验，SEAME是一个200小时的自发普通话-英语语码转换语料库，广泛用于多语言语音研究。SEAME提出了三个主要挑战：(i) 对话自发性：频繁的不流畅（犹豫、重叠、错误起始）和口语韵律；(ii) 电话失真和环境噪声：低比特率信道伪像以及移动或固定电话设置中典型的背景声音；(iii) 跨语言语码转换：话语中普通话和英语的快速交替。
</doc2xLabelTranslationLabelBlock>


Without any fine-tuning, we directly apply our Distinctive Codec model trained on LibriSpeech to a subset of 50 utterances randomly selected from SEAME. The evaluation focuses on PESQ, which serves as the primary perceptual quality metric and correlates strongly with both intelligibility and signal fidelity.
<doc2xLabelTranslationLabelBlock>
在没有任何微调的情况下，我们直接将在LibriSpeech上训练的独特编解码器模型应用于从SEAME中随机选择的50个话语的子集。评估集中在PESQ上，它作为主要的感知质量指标，与可懂度和信号保真度都密切相关。
</doc2xLabelTranslationLabelBlock>


Under a low token rate setting ( \( {9.5}\mathrm{\;{Hz}} \) ),our model achieves a PESQ score of 1.4214, significantly outperforming SpeechTo-kenizer's PESQ score of 1.0695 under the same configuration. This substantial improvement demonstrates that the proposed distinctive feature-based tokenization not only preserves critical acoustic cues in the English domain but also exhibits robust transferability to unseen languages and mixed-lingual acoustic conditions.
<doc2xLabelTranslationLabelBlock>
在低令牌率设置（\( {9.5}\mathrm{\;{Hz}} \)）下，我们的模型实现了1.4214的PESQ分数，在相同配置下显著优于SpeechTo - kenizer的1.0695的PESQ分数。这一显著改进表明，所提出的基于显著特征的令牌化不仅在英语域中保留了关键的声学线索，而且在未见语言和混合语言声学条件下也表现出强大的可转移性。
</doc2xLabelTranslationLabelBlock>


These results provide empirical evidence that Distinctive Codec retains perceptually significant information in cross-lingual and code-switched scenarios, supporting its potential as a universal speech tokenizer across diverse linguistic domains.
<doc2xLabelTranslationLabelBlock>
这些结果提供了实证证据，表明独特编解码器在跨语言和语码转换场景中保留了感知上重要的信息，支持了其作为跨不同语言域的通用语音令牌化器的潜力。
</doc2xLabelTranslationLabelBlock>


## D. Comparison with WaveTokenizer
<doc2xLabelTranslationLabelBlock>
## D. 与WaveTokenizer的比较
</doc2xLabelTranslationLabelBlock>


To further validate the effectiveness of our distinctive feature-based approach, we conducted additional experiments comparing our method with WaveTokenizer [73], another state-of-the-art neural speech codec. While our main results (Table II) demonstrate competitive performance against SpeechTokenizer, this additional comparison provides broader context for our approach within the current landscape of speech tokenization methods.
<doc2xLabelTranslationLabelBlock>
为了进一步验证我们基于独特特征的方法的有效性，我们进行了额外的实验，将我们的方法与另一种最先进的神经语音编解码器WaveTokenizer [73]进行比较。虽然我们的主要结果（表二）显示了与SpeechTokenizer相比具有竞争力的性能，但这种额外的比较为我们的方法在当前语音分词方法的格局中提供了更广泛的背景。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



TABLE V: Performance comparison between WaveTokenizer and our Distinctive Codec at similar token rates. Lower values are better for MEL Error, STFT, and WER; higher values are better for PESQ and STOI.
<doc2xLabelTranslationLabelBlock>
表五：WaveTokenizer和我们的独特编解码器在相似令牌率下的性能比较。对于MEL误差、STFT和WER，值越低越好；对于PESQ和STOI，值越高越好。
</doc2xLabelTranslationLabelBlock>


<table><tr><td rowspan="2">Model</td><td rowspan="2">Token Rate</td><td colspan="5">\( \mathbf{{Metrics}} \)</td></tr><tr><td>\( \mathbf{{MEL} \downarrow  } \)</td><td>STFT↓</td><td>PESQ↑</td><td>STOI↑</td><td>WER↓</td></tr><tr><td>WaveTokenizer</td><td>10 Hz</td><td>0.3139</td><td>1.0459</td><td>1.8333</td><td>0.7449</td><td>0.5535</td></tr><tr><td>Distinctive Codec (GSQ)</td><td>\( {9.5}\mathrm{\;{Hz}} \)</td><td>0.2006</td><td>0.6021</td><td>2.1901</td><td>0.8114</td><td>0.3061</td></tr></table>
<doc2xLabelTranslationLabelBlock>
<table><tbody><tr><td rowspan="2">模型</td><td rowspan="2">令牌速率</td><td colspan="5">\( \mathbf{{Metrics}} \)</td></tr><tr><td>\( \mathbf{{MEL} \downarrow  } \)</td><td>短时傅里叶变换↓</td><td>语音质量感知评估↑</td><td>短时客观可懂度↑</td><td>词错误率↓</td></tr><tr><td>波形分词器</td><td>10赫兹</td><td>0.3139</td><td>1.0459</td><td>1.8333</td><td>0.7449</td><td>0.5535</td></tr><tr><td>独特编解码器(GSQ)</td><td>\( {9.5}\mathrm{\;{Hz}} \)</td><td>0.2006</td><td>0.6021</td><td>2.1901</td><td>0.8114</td><td>0.3061</td></tr></tbody></table>
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



For a fair comparison, we trained WaveTokenizer on the LibriSpeech dataset with a token rate of \( {10}\mathrm{\;{Hz}} \) ,matching the operating conditions of our Distinctive Codec ( \( {9.5}\mathrm{\;{Hz}} \) ). The results are presented in Table V.
<doc2xLabelTranslationLabelBlock>
为了进行公平比较，我们在LibriSpeech数据集上训练了WaveTokenizer，其token率为\( {10}\mathrm{\;{Hz}} \)，与我们的独特编解码器（\( {9.5}\mathrm{\;{Hz}} \)）的操作条件相匹配。结果见表V。
</doc2xLabelTranslationLabelBlock>


Despite operating at a slightly lower token rate, our Distinctive Codec with GSQ significantly outperforms WaveTokenizer across all evaluation metrics. Specifically, our approach reduces MEL Error by 36.1%, STFT distortion by 42.4%, and WER by 44.7%, while improving PESQ by 19.5% and STOI by \( {8.9}\% \) . These substantial improvements further validate the effectiveness of our distinctive feature-based approach and Group-wise Scalar Quantization method.
<doc2xLabelTranslationLabelBlock>
尽管我们的带有GSQ的独特编解码器以略低的token率运行，但在所有评估指标上都显著优于WaveTokenizer。具体而言，我们的方法将MEL误差降低了36.1%，STFT失真降低了42.4%，WER降低了44.7%，同时将PESQ提高了19.5%，将STOI提高了\( {8.9}\% \)。这些显著的改进进一步验证了我们基于独特特征的方法和分组标量量化方法的有效性。
</doc2xLabelTranslationLabelBlock>


## E. Investigate the impact of semantic distillation
<doc2xLabelTranslationLabelBlock>
## E. 研究语义蒸馏的影响
</doc2xLabelTranslationLabelBlock>


TABLE VI: Impact of Semantic Distillation (SD) on speech tokenization models. SD refers to the process of guiding the first RVQ layer with HuBERT representations. Lower values are better for MEL Error, STFT, and WER; higher values are better for PESQ and STOI.
<doc2xLabelTranslationLabelBlock>
表VI：语义蒸馏（SD）对语音tokenization模型的影响。SD指的是用HuBERT表示引导第一个RVQ层的过程。对于MEL误差、STFT和WER，值越低越好；对于PESQ和STOI，值越高越好。
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



<table><tr><td rowspan="2">Model</td><td rowspan="2">Frame Rate</td><td colspan="5">\( \mathbf{{Metrics}} \)</td></tr><tr><td>MEL↓</td><td>STFT↓</td><td>PESQ↑</td><td>STOI↑</td><td>WER↓</td></tr><tr><td>Distinctive Codec</td><td>9.5</td><td>0.286</td><td>1.321</td><td>1.915</td><td>0.768</td><td>0.427</td></tr><tr><td>Distinctive Codec + SD</td><td>9.5</td><td>0.337</td><td>1.660</td><td>1.721</td><td>0.758</td><td>0.359</td></tr><tr><td>Speech Tokenizer</td><td>10</td><td>0.314</td><td>1.046</td><td>1.833</td><td>0.745</td><td>0.554</td></tr><tr><td>Speech Tokenizer + SD</td><td>10</td><td>0.561</td><td>2.242</td><td>1.070</td><td>0.606</td><td>0.999</td></tr><tr><td>Speech Tokenizer</td><td>50</td><td>0.234</td><td>0.565</td><td>2.450</td><td>0.844</td><td>0.170</td></tr><tr><td>Speech Tokenizer + SD</td><td>50</td><td>0.295</td><td>0.750</td><td>2.220</td><td>0.811</td><td>0.110</td></tr></table>
<doc2xLabelTranslationLabelBlock>
<table><tbody><tr><td rowspan="2">模型</td><td rowspan="2">帧率</td><td colspan="5">\( \mathbf{{Metrics}} \)</td></tr><tr><td>梅尔（指标）↓</td><td>短时傅里叶变换（指标）↓</td><td>语音质量感知评估（指标）↑</td><td>短时客观可懂度（指标）↑</td><td>词错误率（指标）↓</td></tr><tr><td>独特编解码器</td><td>9.5</td><td>0.286</td><td>1.321</td><td>1.915</td><td>0.768</td><td>0.427</td></tr><tr><td>独特编解码器 + 标准差</td><td>9.5</td><td>0.337</td><td>1.660</td><td>1.721</td><td>0.758</td><td>0.359</td></tr><tr><td>语音分词器</td><td>10</td><td>0.314</td><td>1.046</td><td>1.833</td><td>0.745</td><td>0.554</td></tr><tr><td>语音分词器 + 标准差</td><td>10</td><td>0.561</td><td>2.242</td><td>1.070</td><td>0.606</td><td>0.999</td></tr><tr><td>语音分词器</td><td>50</td><td>0.234</td><td>0.565</td><td>2.450</td><td>0.844</td><td>0.170</td></tr><tr><td>语音分词器 + 标准差</td><td>50</td><td>0.295</td><td>0.750</td><td>2.220</td><td>0.811</td><td>0.110</td></tr></tbody></table>
</doc2xLabelTranslationLabelBlock>


<!-- Media -->



To investigate the impact of semantic distillation on speech tokenization models, we conducted experiments comparing performance with and without distillation across different frame rates. Table VI presents these results, revealing several important insights not previously reported in the original SpeechTokenizer work.
<doc2xLabelTranslationLabelBlock>
为了研究语义蒸馏对语音分词模型的影响，我们进行了实验，比较了不同帧率下有蒸馏和无蒸馏时的性能。表六展示了这些结果，揭示了一些在原始语音分词器工作中未被报道的重要见解。
</doc2xLabelTranslationLabelBlock>


Our findings demonstrate a consistent pattern: when semantic distillation is applied, reconstruction quality metrics (MEL Error, STFT, PESQ, STOI) tend to decrease, while speech content preservation measured by WER improves. This tradeoff is evident in both our Distinctive Codec at \( {9.5}\mathrm{\;{Hz}} \) and SpeechTokenizer at \( {50}\mathrm{\;{Hz}} \) ,where the addition of semantic distillation increases reconstruction errors but significantly reduces word error rates.
<doc2xLabelTranslationLabelBlock>
我们的研究结果呈现出一种一致的模式：当应用语义蒸馏时，重建质量指标（MEL误差、STFT、PESQ、STOI）往往会下降，而通过WER衡量的语音内容保留情况则会改善。这种权衡在我们位于\( {9.5}\mathrm{\;{Hz}} \)的独特编解码器和位于\( {50}\mathrm{\;{Hz}} \)的语音分词器中都很明显，其中语义蒸馏的加入增加了重建误差，但显著降低了单词错误率。
</doc2xLabelTranslationLabelBlock>


Notably,the comparison between SpeechTokenizer at \( {10}\mathrm{\;{Hz}} \) and Distinctive Codec at \( {9.5}\mathrm{\;{Hz}} \) highlights the superior stability of our approach at lower frame rates. While both models experience changes when semantic distillation is applied, SpeechTokenizer at \( {10}\mathrm{\;{Hz}} \) shows dramatic degradation across all metrics. In contrast, our Distinctive Codec maintains relatively stable performance with more moderate reconstruction quality decreases and substantial WER improvements.
<doc2xLabelTranslationLabelBlock>
值得注意的是，位于\( {10}\mathrm{\;{Hz}} \)的语音分词器和位于\( {9.5}\mathrm{\;{Hz}} \)的独特编解码器之间的比较突出了我们的方法在较低帧率下的卓越稳定性。虽然在应用语义蒸馏时，两个模型都会发生变化，但位于\( {10}\mathrm{\;{Hz}} \)的语音分词器在所有指标上都表现出显著下降。相比之下，我们的独特编解码器保持了相对稳定的性能，重建质量下降较为适度，而WER有显著改善。
</doc2xLabelTranslationLabelBlock>


## VII. CONCLUSION
<doc2xLabelTranslationLabelBlock>
## VII. 结论
</doc2xLabelTranslationLabelBlock>


In this paper, we investigated the effectiveness of distinctive features for speech representation in depression detection, demonstrating that adaptive segmentation aligned with acoustic boundaries preserves critical temporal dynamics. Our experiments address the fundamental limitation of fixed-rate processing that destroys timing information essential for clinical applications. Results show that distinctive feature-based tokenization produces more coherent latent representations with improved codebook utilization, while our Group-wise Scalar Quantization strategy enables stable quantization at low token rates. The substantial performance gains in depression detection (+35.0% relative improvement) validate that preserving temporal structure is crucial for capturing clinically relevant biomarkers such as pause patterns and speech rate variations. This work establishes distinctive features as a promising direction for neural speech codecs in temporally sensitive applications, with potential benefits for clinical assessment systems and other domains requiring faithful preservation of timing dynamics.
<doc2xLabelTranslationLabelBlock>
在本文中，我们研究了独特特征在抑郁症检测中用于语音表示的有效性，证明与声学边界对齐的自适应分割保留了关键的时间动态。我们的实验解决了固定速率处理的根本局限性，即它会破坏临床应用中至关重要的时间信息。结果表明，基于独特特征的分词产生了更连贯的潜在表示，提高了码本利用率，而我们的分组标量量化策略能够在低令牌率下实现稳定量化。抑郁症检测中的显著性能提升（相对提高35.0%）证实，保留时间结构对于捕获临床相关生物标志物（如停顿模式和语速变化）至关重要。这项工作将独特特征确立为神经语音编解码器在时间敏感应用中的一个有前途的方向，对临床评估系统和其他需要忠实保留时间动态的领域具有潜在益处。
</doc2xLabelTranslationLabelBlock>


## ACKNOWLEDGEMENT
<doc2xLabelTranslationLabelBlock>
## 致谢
</doc2xLabelTranslationLabelBlock>


This work was supported by the Australian Research Council Discovery Project DP230101184. REFERENCES
<doc2xLabelTranslationLabelBlock>
这项工作得到了澳大利亚研究理事会发现项目DP230101184的支持。参考文献
</doc2xLabelTranslationLabelBlock>


[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., "Qwen technical report," arXiv preprint arXiv:2309.16609, 2023.
<doc2xLabelTranslationLabelBlock>
[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang等人，“Qwen技术报告”，arXiv预印本arXiv:2309.16609，2023年。
</doc2xLabelTranslationLabelBlock>


[2] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., "The llama 3 herd of models," arXiv preprint arXiv:2407.21783, 2024.
<doc2xLabelTranslationLabelBlock>
[2] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan等人，“模型的llama 3群体”，arXiv预印本arXiv:2407.21783，2024年。
</doc2xLabelTranslationLabelBlock>


[3] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.
<doc2xLabelTranslationLabelBlock>
[3] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale等人，“Llama 2：开放基础和微调聊天模型”，arXiv预印本arXiv:2307.09288，2023年。
</doc2xLabelTranslationLabelBlock>


[4] A. Défossez, L. Mazaré, M. Orsini, A. Royer, P. Pérez, H. Jégou, E. Grave, and N. Zeghidour, "Moshi: a speech-text foundation model for real-time dialogue," arXiv preprint arXiv:2410.00037, 2024.
<doc2xLabelTranslationLabelBlock>
[4] A. Défossez, L. Mazaré, M. Orsini, A. Royer, P. Pérez, H. Jégou, E. Grave, 和N. Zeghidour，“Moshi：用于实时对话的语音-文本基础模型”，arXiv预印本arXiv:2410.00037，2024年。
</doc2xLabelTranslationLabelBlock>


[5] Z. Du, Q. Chen, S. Zhang, K. Hu, H. Lu, Y. Yang, H. Hu, S. Zheng, Y. Gu, Z. Ma et al., "Cosyvoice: A scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens," arXiv preprint arXiv:2407.05407, 2024.
<doc2xLabelTranslationLabelBlock>
[5] Z. Du, Q. Chen, S. Zhang, K. Hu, H. Lu, Y. Yang, H. Hu, S. Zheng, Y. Gu, Z. Ma等人，“Cosyvoice：基于监督语义令牌的可扩展多语言零样本文本到语音合成器”，arXiv预印本arXiv:2407.05407，2024年。
</doc2xLabelTranslationLabelBlock>


[6] Z. Du, Y. Wang, Q. Chen, X. Shi, X. Lv, T. Zhao, Z. Gao, Y. Yang, C. Gao, H. Wang et al., "Cosyvoice 2: Scalable streaming speech synthesis with large language models," arXiv preprint arXiv:2412.10117, 2024.
<doc2xLabelTranslationLabelBlock>
[6] Z. Du, Y. Wang, Q. Chen, X. Shi, X. Lv, T. Zhao, Z. Gao, Y. Yang, C. Gao, H. Wang等人，“Cosyvoice 2：使用大语言模型的可扩展流式语音合成”，arXiv预印本arXiv:2412.10117，2024年。
</doc2xLabelTranslationLabelBlock>


[7] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi et al., "Audiolm: a language modeling approach to audio generation," IEEE/ACM transactions on audio, speech, and language processing, vol. 31, pp. 2523-2533, 2023.
<doc2xLabelTranslationLabelBlock>
[7] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi等人，“Audiolm：一种用于音频生成的语言建模方法”，《IEEE/ACM音频、语音和语言处理汇刊》，第31卷，第2523 - 2533页，2023年。
</doc2xLabelTranslationLabelBlock>


[8] T. A. Nguyen, E. Kharitonov, J. Copet, Y. Adi, W.-N. Hsu, A. Elkahky, P. Tomasello, R. Algayres, B. Sagot, A. Mohamed et al., "Generative spoken dialogue language modeling," Transactions of the Association for Computational Linguistics, vol. 11, pp. 250-266, 2023.
<doc2xLabelTranslationLabelBlock>
[8] T. A. Nguyen, E. Kharitonov, J. Copet, Y. Adi, W.-N. Hsu, A. Elkahky, P. Tomasello, R. Algayres, B. Sagot, A. Mohamed等人，“生成式口语对话语言建模”，《计算语言学协会会刊》，第11卷，第250 - 266页，2023年。
</doc2xLabelTranslationLabelBlock>


[9] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., "On generative spoken language modeling from raw audio," Transactions of the Association for Computational Linguistics, vol. 9, pp. 1336-1354, 2021.
<doc2xLabelTranslationLabelBlock>
[9] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed等人，“关于从原始音频进行生成式口语语言建模”，《计算语言学协会会刊》，第9卷，第1336 - 1354页，2021年。
</doc2xLabelTranslationLabelBlock>